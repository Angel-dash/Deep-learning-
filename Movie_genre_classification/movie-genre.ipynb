{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2347441,"sourceType":"datasetVersion","datasetId":1417162}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-04T14:45:17.039024Z","iopub.execute_input":"2024-03-04T14:45:17.039680Z","iopub.status.idle":"2024-03-04T14:45:17.430587Z","shell.execute_reply.started":"2024-03-04T14:45:17.039645Z","shell.execute_reply":"2024-03-04T14:45:17.429581Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/description.txt\n/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data_solution.txt\n/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data.txt\n/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/train_data.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:17.556645Z","iopub.execute_input":"2024-03-04T14:45:17.557716Z","iopub.status.idle":"2024-03-04T14:45:21.304787Z","shell.execute_reply.started":"2024-03-04T14:45:17.557669Z","shell.execute_reply":"2024-03-04T14:45:21.303829Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-04 14:45:17.906725: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-04 14:45:17.906780: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-04 14:45:17.908337: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:21.306529Z","iopub.execute_input":"2024-03-04T14:45:21.307084Z","iopub.status.idle":"2024-03-04T14:45:21.721671Z","shell.execute_reply.started":"2024-03-04T14:45:21.307054Z","shell.execute_reply":"2024-03-04T14:45:21.720819Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/train_data.txt\",sep=':::', names=['ID', 'TITLE', 'GENRE', 'DESCRIPTION'])\ndisplay(df.head())\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:21.722914Z","iopub.execute_input":"2024-03-04T14:45:21.723221Z","iopub.status.idle":"2024-03-04T14:45:22.120290Z","shell.execute_reply.started":"2024-03-04T14:45:21.723195Z","shell.execute_reply":"2024-03-04T14:45:22.119267Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_441/3152433989.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  df = pd.read_csv(\"/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/train_data.txt\",sep=':::', names=['ID', 'TITLE', 'GENRE', 'DESCRIPTION'])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   ID                               TITLE       GENRE  \\\n0   1       Oscar et la dame rose (2009)       drama    \n1   2                       Cupid (1997)    thriller    \n2   3   Young, Wild and Wonderful (1980)       adult    \n3   4              The Secret Sin (1915)       drama    \n4   5             The Unrecovered (2007)       drama    \n\n                                         DESCRIPTION  \n0   Listening in to a conversation between his do...  \n1   A brother and sister with a past incestuous r...  \n2   As the bus empties the students for their fie...  \n3   To help their unemployed father make ends mee...  \n4   The film's title refers not only to the un-re...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>TITLE</th>\n      <th>GENRE</th>\n      <th>DESCRIPTION</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Oscar et la dame rose (2009)</td>\n      <td>drama</td>\n      <td>Listening in to a conversation between his do...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Cupid (1997)</td>\n      <td>thriller</td>\n      <td>A brother and sister with a past incestuous r...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Young, Wild and Wonderful (1980)</td>\n      <td>adult</td>\n      <td>As the bus empties the students for their fie...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>The Secret Sin (1915)</td>\n      <td>drama</td>\n      <td>To help their unemployed father make ends mee...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>The Unrecovered (2007)</td>\n      <td>drama</td>\n      <td>The film's title refers not only to the un-re...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"(54214, 4)\n","output_type":"stream"}]},{"cell_type":"code","source":"'''df_test=pd.read_csv(\"/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data.txt\", sep=':::',names=['ID','TITLE','GENRE','DESCRIPTIONS'])\ndisplay(train_data.head())\nprint(test_data.shape)'''","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:22.122257Z","iopub.execute_input":"2024-03-04T14:45:22.122560Z","iopub.status.idle":"2024-03-04T14:45:22.129368Z","shell.execute_reply.started":"2024-03-04T14:45:22.122535Z","shell.execute_reply":"2024-03-04T14:45:22.128394Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'df_test=pd.read_csv(\"/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data.txt\", sep=\\':::\\',names=[\\'ID\\',\\'TITLE\\',\\'GENRE\\',\\'DESCRIPTIONS\\'])\\ndisplay(train_data.head())\\nprint(test_data.shape)'"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df[0:1]","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:22.130620Z","iopub.execute_input":"2024-03-04T14:45:22.131008Z","iopub.status.idle":"2024-03-04T14:45:22.147189Z","shell.execute_reply.started":"2024-03-04T14:45:22.130973Z","shell.execute_reply":"2024-03-04T14:45:22.146296Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   ID                           TITLE    GENRE  \\\n0   1   Oscar et la dame rose (2009)    drama    \n\n                                         DESCRIPTION  \n0   Listening in to a conversation between his do...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>TITLE</th>\n      <th>GENRE</th>\n      <th>DESCRIPTION</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Oscar et la dame rose (2009)</td>\n      <td>drama</td>\n      <td>Listening in to a conversation between his do...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:22.148417Z","iopub.execute_input":"2024-03-04T14:45:22.148715Z","iopub.status.idle":"2024-03-04T14:45:22.166079Z","shell.execute_reply.started":"2024-03-04T14:45:22.148692Z","shell.execute_reply":"2024-03-04T14:45:22.165040Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"          ID                                         TITLE          GENRE  \\\n0          1                 Oscar et la dame rose (2009)          drama    \n1          2                                 Cupid (1997)       thriller    \n2          3             Young, Wild and Wonderful (1980)          adult    \n3          4                        The Secret Sin (1915)          drama    \n4          5                       The Unrecovered (2007)          drama    \n...      ...                                           ...            ...   \n54209  54210                              \"Bonino\" (1953)         comedy    \n54210  54211                  Dead Girls Don't Cry (????)         horror    \n54211  54212    Ronald Goedemondt: Ze bestaan echt (2008)    documentary    \n54212  54213                     Make Your Own Bed (1944)         comedy    \n54213  54214   Nature's Fury: Storm of the Century (2006)        history    \n\n                                             DESCRIPTION  \n0       Listening in to a conversation between his do...  \n1       A brother and sister with a past incestuous r...  \n2       As the bus empties the students for their fie...  \n3       To help their unemployed father make ends mee...  \n4       The film's title refers not only to the un-re...  \n...                                                  ...  \n54209   This short-lived NBC live sitcom centered on ...  \n54210   The NEXT Generation of EXPLOITATION. The sist...  \n54211   Ze bestaan echt, is a stand-up comedy about g...  \n54212   Walter and Vivian live in the country and hav...  \n54213   On Labor Day Weekend, 1935, the most intense ...  \n\n[54214 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>TITLE</th>\n      <th>GENRE</th>\n      <th>DESCRIPTION</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Oscar et la dame rose (2009)</td>\n      <td>drama</td>\n      <td>Listening in to a conversation between his do...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Cupid (1997)</td>\n      <td>thriller</td>\n      <td>A brother and sister with a past incestuous r...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Young, Wild and Wonderful (1980)</td>\n      <td>adult</td>\n      <td>As the bus empties the students for their fie...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>The Secret Sin (1915)</td>\n      <td>drama</td>\n      <td>To help their unemployed father make ends mee...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>The Unrecovered (2007)</td>\n      <td>drama</td>\n      <td>The film's title refers not only to the un-re...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>54209</th>\n      <td>54210</td>\n      <td>\"Bonino\" (1953)</td>\n      <td>comedy</td>\n      <td>This short-lived NBC live sitcom centered on ...</td>\n    </tr>\n    <tr>\n      <th>54210</th>\n      <td>54211</td>\n      <td>Dead Girls Don't Cry (????)</td>\n      <td>horror</td>\n      <td>The NEXT Generation of EXPLOITATION. The sist...</td>\n    </tr>\n    <tr>\n      <th>54211</th>\n      <td>54212</td>\n      <td>Ronald Goedemondt: Ze bestaan echt (2008)</td>\n      <td>documentary</td>\n      <td>Ze bestaan echt, is a stand-up comedy about g...</td>\n    </tr>\n    <tr>\n      <th>54212</th>\n      <td>54213</td>\n      <td>Make Your Own Bed (1944)</td>\n      <td>comedy</td>\n      <td>Walter and Vivian live in the country and hav...</td>\n    </tr>\n    <tr>\n      <th>54213</th>\n      <td>54214</td>\n      <td>Nature's Fury: Storm of the Century (2006)</td>\n      <td>history</td>\n      <td>On Labor Day Weekend, 1935, the most intense ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>54214 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"Genre_df = df.loc[df['GENRE'] == 'horror']\nGenre_df","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:22.168078Z","iopub.execute_input":"2024-03-04T14:45:22.168948Z","iopub.status.idle":"2024-03-04T14:45:22.189790Z","shell.execute_reply.started":"2024-03-04T14:45:22.168918Z","shell.execute_reply":"2024-03-04T14:45:22.188975Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [ID, TITLE, GENRE, DESCRIPTION]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>TITLE</th>\n      <th>GENRE</th>\n      <th>DESCRIPTION</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Get the index of the description with the maximum length\nmax_length_index = df['DESCRIPTION'].str.len().idxmax()\n\n# Get the movie description with the maximum length\nlongest_description = df.loc[max_length_index, 'DESCRIPTION']\n\n# Print the description and its length\nprint(f\"Movie with the longest description:\\n{longest_description}\\nLength: {len(longest_description)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:23.021912Z","iopub.execute_input":"2024-03-04T14:45:23.022296Z","iopub.status.idle":"2024-03-04T14:45:23.067812Z","shell.execute_reply.started":"2024-03-04T14:45:23.022265Z","shell.execute_reply":"2024-03-04T14:45:23.066886Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Movie with the longest description:\n Guy Gabaldon died on August 31, 2006 and the world lost someone very special. During the bloody struggle for Saipan in July 1944, U.S. Marine PFC Guy Gabaldon is indeed officially credited with capturing over 1500 Japanese soldiers and civilians - singlehandedly, a record that is untouchable in the annals of American military history. For over sixty years, Guy talked about his exploits on that island, sharing his experience and using his celebrity to inspire new generations who valued bravery and bravado. However, war experience alone does not make a life, and Guy's didn't stop in 1944. He lived many different lives and most importantly he took it upon himself to help the less fortunate, particularly the wayward teenagers he encountered when he returned to the Mariana Islands in 1980, where he would live for twenty years. Guy Gabaldon grew up in East Los Angeles where he spent more time on the streets than at home. He would get into fights and he was thrown out of school at one point, but things began to change when he was introduced to the Japanese American community. Practically adopted by his Nisei school friends, Guy learned about the Japanese culture, its language, and the tight family structure that was alien to him. All of these elements - learned at first hand - would have a dramatic effect on his experiences on Saipan. When his Japanese American friends were interned after Pearl Harbor, Guy, 17, joined the Marine Corps, trained at Camp Pendleton, and was assigned as a scout to the 2nd Regiment of the 2nd Marine Division. His unit was then shipped to Hawaii, and then on into the Central Pacific, where he landed on Saipan, nine days after D-Day in Europe. Saipan was a rocky, cave-strewn island in the Mariana chain. It was part of the Japanese Empire's inner defense perimeter and it had an airfield within striking distance of Japan. It also had a large civilian population of Japanese and native islanders. The American high command in the Pacific had determined that the Marianas - Guam, Tinian and Saipan were a high priority for the war effort. B-29s were now flying and they needed a base to attack Japan. Saipan fit that bill. Guy Gabaldon didn't set out to be a hero. In the first few days of heavy fighting, he simply tried to survive murderous mortar, artillery and machine gun fire. But in succeeding days, he began to go on lone-wolf excursions into the countryside and he brought prisoners back. Japanese prisoners were a bit of an oddity at that time. The credo of most soldiers of the Japanese Army was kill or be killed. Japanese soldiers on Saipan were ordered to kill seven Marines for one Japanese. Thus, the campaign featured one suicide banzai charge after another. Capturing one Japanese was considered a feat - bringing in 1500 was unthinkable. But, amazingly, that's exactly what Guy Gabaldon did during the two months of early fighting on Saipan. At one point, he captured 800 in one day - his commanding officer Captain John Schwabe would later dub him \"The Pied Piper of Saipan.\" How did Guy do it? Perhaps it was his language skill - Guy was hardly fluent in Japanese but he spoke the language with a certain inflection that reached into the psyche of the exhausted, hopelessly outnumbered island garrison. He had learned the words on the streets of L.A. with his Japanese-American friends, and those words helped him on the island. Perhaps it was the fact that the Japanese were, in the end, human beings who just couldn't fight anymore. Timing was thus everything. Guy wasn't hesitant to make a point with a hand grenade of a carbine if the enemy proved stubborn. But they eventually came out of their caves and became his prisoner. Guy was later wounded after the island was secured. Astonishingly, he was denied the Congressional Medal of Honor - a medal for which he was recommended by Captain Schwabe of the 2nd Marines. He did receive the Silver Star for his valor, but he was not promoted and left the Marine service as a PFC. Being Hispanic, perhaps, didn't help his cause. Racism and prejudice was rife throughout the U.S. armed forces in World War II and Guy was not immune to it. Guy returned to the United States, married a Japanese woman who was living in Mexico and became a successful pilot and importer. His story was first told on the television program \"This is your Life\" in the late 1950s. That program came to the attention of Hollywood and a movie was produced in 1960 entitled \"Hell to Eternity.\" Actor Jeffrey Hunter played Guy. Hunter was your poster boy U.S. Marine - no reference was ever made to his Guy's Hispanic ethnicity. However, the notoriety of the film at that time encouraged the U.S. Navy to award Guy its highest decoration - the Navy Cross. But no Medal of Honor. Today, a strong effort is being waged by Congressmen, private business people and friends of Guy to get him the Medal. It would be measured as a sign of respect, not only to Guy, but to the people in America of Hispanic descent. Production Production on \"East L.A. Marine\" commenced in late 2003. Guy Gabaldon was enlisted as a creative partner on the project, and he was interviewed, at length, at his home outside Gainsville, Florida. We later interviewed his commanding officer, U.S. Marine Colonel John Schwabe, at his winter home in Tucson, Arizona. In June 2004, during the 60th Anniversary celebrations on Saipan, a local DV crew was hired and footage was gathered all over the island. Seventeen additional interviews were completed with returning veterans, local historians and friends of Guys. Guy and his wife had returned to the Island and lived there for many years - so he was well known throughout the Marianas. His autobiographical book Maverick Marine was published in 1990. Much of the footage that was gathered on the island is designed to match combat footage and still photographs taken of the campaign (yesterday and today shots). In early 2005, we interviewed a number of Hispanic veterans in Montebello, California. That May, we met Guy in Corpus Christi, Texas and helped celebrate Memorial Day with him. Footage of Guy participating in solemn commemorative ceremonies combined with nostalgic trips to the U.S.S. Lexington - a U.S. Essex-class aircraft carrier, that participated in the invasion of Saipan, sixty one years ago. In July 2006, Guy was honored by Mayor Antonio Villagairosa and the Los Angeles City Council. He was indeed a \"favorite son\" of the City of Los Angeles. This is the story of an extraordinary man - Guy Gabaldon - a U.S. Marine of Hispanic descent who single-handedly captured over 1100 Japanese during the bloody fighting on Saipan in the summer of 1944. One of the legendary heroes of World War II, Guy Gabaldon's true story is now told for the very first time in EAST L.A. MARINE: THE UNTOLD TRUE STORY OF GUY GABALDON. Guy Gabaldon's roots date back to the time of the Spanish conquistadors in New Mexico. Born in Los Angeles, Guy grew up in Boyle Heights where he spent more time on the streets than at home. He would get into fights and he was thrown out of school at one point, but things began to change when he was introduced to the Japanese American community. Practically adopted by his Nisei school friends, Guy learned about the Japanese culture, its language, and the tight family structure that was alien to him. All of these elements - learned at first hand - would have a dramatic effect on his experiences on Saipan. When his Japanese American friends were interned after Pearl Harbor, Guy, 17, joined the Marine Corps, trained at Camp Pendleton, and was assigned as a scout to the 2nd Regiment of the 2nd Marine Division. His combat unit landed on Saipan, nine days after D-Day in Europe. Saipan was a rocky, cave-strewn island in the Mariana chain. Guy Gabaldon didn't set out to be a hero. In the first few days of heavy fighting, he simply tried to survive murderous mortar, artillery and machine gun fire. But in succeeding days, he began to go on lone-wolf excursions into the countryside and he brought prisoners back. Japanese prisoners were an oddity at that time. The credo of most soldiers of the Japanese Army was kill or be killed. Japanese soldiers on Saipan were ordered to kill seven Marines for one Japanese. Thus, the campaign featured one suicide banzai charge after another. Capturing one Japanese was considered a feat - bringing in over 1500 was unthinkable. But, amazingly, that's exactly what Guy Gabaldon did during the two months of early fighting on Saipan. At one point, he captured 800 in one day - his commanding officer Captain John Schwabe would later dub him \"The Pied Piper of Saipan.\" Guy was later wounded after the island was secured. Astonishingly, he was denied the Congressional Medal of Honor - a medal for which he was recommended by Captain Schwabe of the 2nd Marines. He did receive the Silver Star for his valor, but he was not promoted and left the Marine service as a PFC. Being Hispanic, perhaps, didn't help his cause. Racism and prejudice was rife throughout the U.S. armed forces in World War II and Guy was not immune to it. Fifteen years after the end of World War II, Guy's story was dramatized in the 1960 war film, HELL TO ETERNITY. Since there were no Hispanic leading men at that time, producer Irvin Levin decided to cast Jeffrey Hunter as Guy. Hunter was 6'1\" and Caucasian, a poster-boy Marine type. In reality, Guy \"Gabby\" Gabaldon was 5' 3 3/4\" and of Hispanic descent - an important facet of Guy's life that was ignored in the film. Although nicely written by Ted Sherdeman and directed in slam bang style by Phil Karlson (the movie was the first to feature on-screen squib bullet hits on bodies), much of the story was invented and the manner in which General Matsui (\"Bridge on the River Kwai's\" Sessue Hayakawa) personally ordered the whole island to surrender to Guy was ludicrous. One positive aspect of HELL TO ETERNITY was that it woke up the Marine Corps to Gabaldon's contribution and he was awarded the Navy Cross at El Toro Marine Air Station in 1960. However, any effort to get the Corps to endorse the Congressional Medal of Honor was stymied. Guy Lewis Gabaldon died of a heart attack on August 31, 2006. He was 80. Today, a strong effort is being waged by Congressmen, private business people and friends of Guy to get him the Medal posthumously. It would be measured as a sign of respect, not only to Guy, but to the people in America of Hispanic descent who fought in World War II. ###\nLength: 10504\n","output_type":"stream"}]},{"cell_type":"code","source":"average_length = df['DESCRIPTION'].str.len().mean()\n\nprint(f\"The average length of movie descriptions is: {average_length:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:23.698250Z","iopub.execute_input":"2024-03-04T14:45:23.698577Z","iopub.status.idle":"2024-03-04T14:45:23.741101Z","shell.execute_reply.started":"2024-03-04T14:45:23.698553Z","shell.execute_reply":"2024-03-04T14:45:23.740018Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"The average length of movie descriptions is: 600.45\n","output_type":"stream"}]},{"cell_type":"code","source":"max_length=800\ntrunc_type='post'\npadding_type='post'\noov_tok = \"\"","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:24.430971Z","iopub.execute_input":"2024-03-04T14:45:24.431747Z","iopub.status.idle":"2024-03-04T14:45:24.436060Z","shell.execute_reply.started":"2024-03-04T14:45:24.431712Z","shell.execute_reply":"2024-03-04T14:45:24.435015Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"num_classes = df['GENRE'].nunique()\nprint(f\"Number of classes: {num_classes}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:25.057484Z","iopub.execute_input":"2024-03-04T14:45:25.057844Z","iopub.status.idle":"2024-03-04T14:45:25.067249Z","shell.execute_reply.started":"2024-03-04T14:45:25.057816Z","shell.execute_reply":"2024-03-04T14:45:25.066228Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Number of classes: 27\n","output_type":"stream"}]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:25.692467Z","iopub.execute_input":"2024-03-04T14:45:25.692818Z","iopub.status.idle":"2024-03-04T14:45:25.719292Z","shell.execute_reply.started":"2024-03-04T14:45:25.692791Z","shell.execute_reply":"2024-03-04T14:45:25.718210Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"ID             0\nTITLE          0\nGENRE          0\nDESCRIPTION    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Data pre processing","metadata":{}},{"cell_type":"markdown","source":"#Step 1 converting into lower cases, so that two words with such as Ball and ball will be considered as singel token\n","metadata":{}},{"cell_type":"code","source":"import re\ndef clean_text(text):\n    # Remove special characters, punctuation, and symbols\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:27.380955Z","iopub.execute_input":"2024-03-04T14:45:27.381363Z","iopub.status.idle":"2024-03-04T14:45:27.386488Z","shell.execute_reply.started":"2024-03-04T14:45:27.381330Z","shell.execute_reply":"2024-03-04T14:45:27.385403Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df['DESCRIPTION_1']=df['DESCRIPTION'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:28.282712Z","iopub.execute_input":"2024-03-04T14:45:28.283400Z","iopub.status.idle":"2024-03-04T14:45:29.685538Z","shell.execute_reply.started":"2024-03-04T14:45:28.283366Z","shell.execute_reply":"2024-03-04T14:45:29.684610Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df['DESCRIPTION_1']","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:29.687354Z","iopub.execute_input":"2024-03-04T14:45:29.687739Z","iopub.status.idle":"2024-03-04T14:45:29.697567Z","shell.execute_reply.started":"2024-03-04T14:45:29.687704Z","shell.execute_reply":"2024-03-04T14:45:29.696464Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0         Listening in to a conversation between his do...\n1         A brother and sister with a past incestuous r...\n2         As the bus empties the students for their fie...\n3         To help their unemployed father make ends mee...\n4         The films title refers not only to the unreco...\n                               ...                        \n54209     This shortlived NBC live sitcom centered on B...\n54210     The NEXT Generation of EXPLOITATION The siste...\n54211     Ze bestaan echt is a standup comedy about gro...\n54212     Walter and Vivian live in the country and hav...\n54213     On Labor Day Weekend 1935 the most intense hu...\nName: DESCRIPTION_1, Length: 54214, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:29.724149Z","iopub.execute_input":"2024-03-04T14:45:29.724816Z","iopub.status.idle":"2024-03-04T14:45:29.876270Z","shell.execute_reply.started":"2024-03-04T14:45:29.724782Z","shell.execute_reply":"2024-03-04T14:45:29.875019Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\ndef remove_stopwords(text):\n    stop_words = set(stopwords.words('english'))\n    new_text = [word for word in text.split() if word not in stop_words]\n    return ' '.join(new_text)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:31.482217Z","iopub.execute_input":"2024-03-04T14:45:31.482578Z","iopub.status.idle":"2024-03-04T14:45:31.488345Z","shell.execute_reply.started":"2024-03-04T14:45:31.482552Z","shell.execute_reply":"2024-03-04T14:45:31.487306Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Removing html tags if there are any","metadata":{}},{"cell_type":"code","source":"df['DESCRIPTION_1']=df['DESCRIPTION_1'].apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:34.921015Z","iopub.execute_input":"2024-03-04T14:45:34.921961Z","iopub.status.idle":"2024-03-04T14:45:43.960943Z","shell.execute_reply.started":"2024-03-04T14:45:34.921920Z","shell.execute_reply":"2024-03-04T14:45:43.960104Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df['DESCRIPTION_1']","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:43.962481Z","iopub.execute_input":"2024-03-04T14:45:43.962906Z","iopub.status.idle":"2024-03-04T14:45:43.970785Z","shell.execute_reply.started":"2024-03-04T14:45:43.962874Z","shell.execute_reply":"2024-03-04T14:45:43.969787Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0        Listening conversation doctor parents 10yearol...\n1        A brother sister past incestuous relationship ...\n2        As bus empties students field trip Museum Natu...\n3        To help unemployed father make ends meet Edith...\n4        The films title refers unrecovered bodies grou...\n                               ...                        \n54209    This shortlived NBC live sitcom centered Bonin...\n54210    The NEXT Generation EXPLOITATION The sisters K...\n54211    Ze bestaan echt standup comedy growing facing ...\n54212    Walter Vivian live country difficult time keep...\n54213    On Labor Day Weekend 1935 intense hurricane ev...\nName: DESCRIPTION_1, Length: 54214, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"import spacy \nnlp=spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:43.971881Z","iopub.execute_input":"2024-03-04T14:45:43.972171Z","iopub.status.idle":"2024-03-04T14:45:49.501365Z","shell.execute_reply.started":"2024-03-04T14:45:43.972129Z","shell.execute_reply":"2024-03-04T14:45:49.500515Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import re \ndef tokenize(text):\n    tokens=re.split('\\W+',text)\n    return tokens\ndf['DESCRIPTION_1']=df['DESCRIPTION_1'].apply(tokenize)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:49.503689Z","iopub.execute_input":"2024-03-04T14:45:49.504292Z","iopub.status.idle":"2024-03-04T14:45:51.560416Z","shell.execute_reply.started":"2024-03-04T14:45:49.504264Z","shell.execute_reply":"2024-03-04T14:45:51.559604Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df['DESCRIPTION_1'].head()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:51.561556Z","iopub.execute_input":"2024-03-04T14:45:51.561859Z","iopub.status.idle":"2024-03-04T14:45:51.571759Z","shell.execute_reply.started":"2024-03-04T14:45:51.561833Z","shell.execute_reply":"2024-03-04T14:45:51.570732Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0    [Listening, conversation, doctor, parents, 10y...\n1    [A, brother, sister, past, incestuous, relatio...\n2    [As, bus, empties, students, field, trip, Muse...\n3    [To, help, unemployed, father, make, ends, mee...\n4    [The, films, title, refers, unrecovered, bodie...\nName: DESCRIPTION_1, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\n\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:51.572966Z","iopub.execute_input":"2024-03-04T14:45:51.573708Z","iopub.status.idle":"2024-03-04T14:45:51.581726Z","shell.execute_reply.started":"2024-03-04T14:45:51.573679Z","shell.execute_reply":"2024-03-04T14:45:51.580723Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def lemmatize_text(text):\n    \"\"\"\n    Lemmatizes a list of tokens using the WordNet dictionary.\n\n    Args:\n        text (list): A list of tokens (words) to be lemmatized.\n\n    Returns:\n        list: A list of lemmatized tokens.\n    \"\"\"\n\n    nltk.download('wordnet')  # Download WordNet if not already available\n\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_text = [lemmatizer.lemmatize(word, pos='v') for word in text]  # Default POS is verb\n    return lemmatized_text\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:51.583031Z","iopub.execute_input":"2024-03-04T14:45:51.583420Z","iopub.status.idle":"2024-03-04T14:45:51.592747Z","shell.execute_reply.started":"2024-03-04T14:45:51.583389Z","shell.execute_reply":"2024-03-04T14:45:51.591838Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import nltk\nprint(nltk.data.path)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:51.593767Z","iopub.execute_input":"2024-03-04T14:45:51.594051Z","iopub.status.idle":"2024-03-04T14:45:51.607071Z","shell.execute_reply.started":"2024-03-04T14:45:51.594028Z","shell.execute_reply":"2024-03-04T14:45:51.605914Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"['/root/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming your dataframe is named 'df' and the column with tokens is 'DESCRIPTION_1'\ndf['DESCRIPTION_2'] = df['DESCRIPTION_1'].apply(lemmatize_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T14:45:51.608060Z","iopub.execute_input":"2024-03-04T14:45:51.608440Z","iopub.status.idle":"2024-03-04T14:45:53.281893Z","shell.execute_reply.started":"2024-03-04T14:45:51.608408Z","shell.execute_reply":"2024-03-04T14:45:53.280215Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming your dataframe is named 'df' and the column with tokens is 'DESCRIPTION_1'\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDESCRIPTION_2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDESCRIPTION_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemmatize_text\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4780\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n","File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","Cell \u001b[0;32mIn[25], line 15\u001b[0m, in \u001b[0;36mlemmatize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Download WordNet if not already available\u001b[39;00m\n\u001b[1;32m     14\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m---> 15\u001b[0m lemmatized_text \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text]  \u001b[38;5;66;03m# Default POS is verb\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lemmatized_text\n","Cell \u001b[0;32mIn[25], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Download WordNet if not already available\u001b[39;00m\n\u001b[1;32m     14\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m---> 15\u001b[0m lemmatized_text \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text]  \u001b[38;5;66;03m# Default POS is verb\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lemmatized_text\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/stem/wordnet.py:40\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, pos\u001b[38;5;241m=\u001b[39mNOUN):\n\u001b[0;32m---> 40\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"],"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************","output_type":"error"}]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n#df[\"Description_1\"] = df[\"DESCRIPTION_1\"].apply(lambda text: \" \".join([lemmatiazer.lemmatize(token) for token in nltk.word_tokenize(text)]))\ndef lemmatize(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Description_1']=df['DESCRIPTION_1'].apply(lemmatize)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # ps=PorterStemmer()\n# def steam_words(text):\n#     return \" \".join([ps.stem(word) for word in text])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['DESCRIPTION_1'].apply(steam_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n# Join the lists into strings\ndf['DESCRIPTION_1'] = df['DESCRIPTION_1'].apply(lambda x: ' '.join(x))\n\n# Now you can fit and transform the data with TfidfVectorizer\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['DESCRIPTION_1']).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tfidf)\nprint(tfidf.get_feature_names_out())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX=df['DESCRIPTION_1']\ny=df['GENRE']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder=LabelEncoder()\ny_train_label=label_encoder.fit_transform(y_train)\ny_test_label=label_encoder.transform(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_label.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=100)  \nX_train_vectorized = vectorizer.fit_transform(X_train).toarray()\nprint(X_train_vectorized)\nX_test_vectorized = vectorizer.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_vectorized.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_tokens = df['DESCRIPTION_1'].apply(lambda x: len(word_tokenize(x))).max()\nmax_tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Bidirectional, LSTM, Dense, Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = MLPClassifier()\n\n# # Train the model\n# model.fit(X_train_vectorized, y_train_label)\n\n# # Predict on test data\n# y_pred = model.predict(X_test_vectorized)\n\n# # Calculate accuracy\n# accuracy = accuracy_score(y_test_label, y_pred)\n# print(\"Accuracy:\", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using LSTM network ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_train_vectorized_dense = X_train_vectorized.toarray()\nX_test_vectorized_dense = X_test_vectorized.toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_vectorized.shape, y_train_label.shape)\nprint(X_test_vectorized.shape, y_test_label.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_padded = pad_sequences(X_train_vectorized, maxlen=max_length, padding='post')\nX_test_padded = pad_sequences(X_test_vectorized_dense, maxlen=max_length, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training_padded = np.array(X_train_padded)\n# training_labels = np.array(y_train_label)\n# testing_padded = np.array(X_test_padded)\n# testing_labels = np.array(y_test_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import constraints\n\n# Attention Layer\nclass AttentionWithContext(Layer):\n    \"\"\"\n    Attention operation, with a context/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    How to use:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Note: The layer has been tested with Keras 2.0.6\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n        # next add a Dense layer (for classification/regression) or whatever...\n    \"\"\"\n\n    def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]\n\ndef dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatible with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size=60000\nembedding_dim=32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, 128, input_length=max_length),  # Experiment with embedding_dim\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),  # Consider LSTMs for sequential data\n    AttentionWithContext(), \n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    #tf.keras.layers.Dropout(0.8),  \n    tf.keras.layers.Dense(512, activation='relu'),  # Adjust neuron count as needed\n    tf.keras.layers.Dense(27, activation='softmax')  # 27 output neurons + softmax for multiclass\n])\n\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_padded.shape, y_train_label.shape)\nprint(X_test_padded.shape, y_test_label.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Assuming y_train_label and y_test_label are  1D arrays with integer labels\ny_train_label_encoded = np.eye(27)[y_train_label]\ny_test_label_encoded = np.eye(27)[y_test_label]\n\n# Now y_train_label_encoded and y_test_label_encoded are  2D arrays with one-hot encoded labels\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_padded.shape, y_train_label_encoded.shape)\nprint(X_test_padded.shape, y_test_label_encoded.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=1000) \n\n# Fit and transform the training and test text data\nX_train_vectorized = vectorizer.fit_transform(X_train)\nX_test_vectorized = vectorizer.transform(X_test)\n\n# Convert sparse matrices to dense arrays\nX_train_dense = X_train_vectorized.toarray()\nX_test_dense = X_test_vectorized.toarray()\n\n# Define maximum sequence length based on the maximum number of tokens\n#max_length = 800  \n\n# Pad sequences to ensure uniform length\nX_train_padded = pad_sequences(X_train_dense, maxlen=max_length, padding='post')\nX_test_padded = pad_sequences(X_test_dense, maxlen=max_length, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Attention mechanism","metadata":{}},{"cell_type":"code","source":"from keras.utils import to_categorical\n\n# Assuming y_train_label and y_test_label are your labels\ny_train_label_one_hot = to_categorical(y_train_label, num_classes=27)\ny_test_label_one_hot = to_categorical(y_test_label, num_classes=27)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_label_one_hot[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_label_one_hot[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.svm import SVC\n# model = SVC(kernel='linear', C=1.0)\n# model.fit(X_train_padded, y_train_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions = model.predict(X_test_padded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import accuracy_score\n# accuracy = accuracy_score(y_test, predictions)\n# print(f\"Accuracy: {accuracy:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 20\nhistory = model.fit(X_train_padded, y_train_label_encoded,batch_size=128, epochs=num_epochs, validation_data=(X_test_padded, y_test_label_encoded), verbose=2) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}