{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2347441,"sourceType":"datasetVersion","datasetId":1417162}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-14T10:11:11.810277Z","iopub.execute_input":"2024-03-14T10:11:11.810681Z","iopub.status.idle":"2024-03-14T10:11:12.166239Z","shell.execute_reply.started":"2024-03-14T10:11:11.810651Z","shell.execute_reply":"2024-03-14T10:11:12.165294Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/description.txt\n/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data_solution.txt\n/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data.txt\n/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/train_data.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:11:12.318934Z","iopub.execute_input":"2024-03-14T10:11:12.319472Z","iopub.status.idle":"2024-03-14T10:11:16.303092Z","shell.execute_reply.started":"2024-03-14T10:11:12.319444Z","shell.execute_reply":"2024-03-14T10:11:16.301814Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-14 10:11:12.733369: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 10:11:12.733487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 10:11:12.738568: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:11:16.305466Z","iopub.execute_input":"2024-03-14T10:11:16.306275Z","iopub.status.idle":"2024-03-14T10:11:16.537637Z","shell.execute_reply.started":"2024-03-14T10:11:16.306241Z","shell.execute_reply":"2024-03-14T10:11:16.536431Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/train_data.txt\",sep=':::', names=['ID', 'TITLE', 'GENRE', 'DESCRIPTION'])\ndisplay(df.head())\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:11:16.539905Z","iopub.execute_input":"2024-03-14T10:11:16.540439Z","iopub.status.idle":"2024-03-14T10:11:17.101780Z","shell.execute_reply.started":"2024-03-14T10:11:16.540405Z","shell.execute_reply":"2024-03-14T10:11:17.101095Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_303/3152433989.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  df = pd.read_csv(\"/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/train_data.txt\",sep=':::', names=['ID', 'TITLE', 'GENRE', 'DESCRIPTION'])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   ID                               TITLE       GENRE  \\\n0   1       Oscar et la dame rose (2009)       drama    \n1   2                       Cupid (1997)    thriller    \n2   3   Young, Wild and Wonderful (1980)       adult    \n3   4              The Secret Sin (1915)       drama    \n4   5             The Unrecovered (2007)       drama    \n\n                                         DESCRIPTION  \n0   Listening in to a conversation between his do...  \n1   A brother and sister with a past incestuous r...  \n2   As the bus empties the students for their fie...  \n3   To help their unemployed father make ends mee...  \n4   The film's title refers not only to the un-re...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>TITLE</th>\n      <th>GENRE</th>\n      <th>DESCRIPTION</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Oscar et la dame rose (2009)</td>\n      <td>drama</td>\n      <td>Listening in to a conversation between his do...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Cupid (1997)</td>\n      <td>thriller</td>\n      <td>A brother and sister with a past incestuous r...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Young, Wild and Wonderful (1980)</td>\n      <td>adult</td>\n      <td>As the bus empties the students for their fie...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>The Secret Sin (1915)</td>\n      <td>drama</td>\n      <td>To help their unemployed father make ends mee...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>The Unrecovered (2007)</td>\n      <td>drama</td>\n      <td>The film's title refers not only to the un-re...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"(54214, 4)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"import re\ndef clean_text(text):\n    # Remove special characters, punctuation, and symbols\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text","metadata":{}},{"cell_type":"code","source":"df[0:1]","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:11:17.474648Z","iopub.execute_input":"2024-03-14T10:11:17.474999Z","iopub.status.idle":"2024-03-14T10:11:17.484710Z","shell.execute_reply.started":"2024-03-14T10:11:17.474975Z","shell.execute_reply":"2024-03-14T10:11:17.483954Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   ID                           TITLE    GENRE  \\\n0   1   Oscar et la dame rose (2009)    drama    \n\n                                         DESCRIPTION  \n0   Listening in to a conversation between his do...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>TITLE</th>\n      <th>GENRE</th>\n      <th>DESCRIPTION</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Oscar et la dame rose (2009)</td>\n      <td>drama</td>\n      <td>Listening in to a conversation between his do...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.drop(columns=['ID','TITLE'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:11:18.306023Z","iopub.execute_input":"2024-03-14T10:11:18.306459Z","iopub.status.idle":"2024-03-14T10:11:18.313994Z","shell.execute_reply.started":"2024-03-14T10:11:18.306429Z","shell.execute_reply":"2024-03-14T10:11:18.312855Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:11:23.909192Z","iopub.execute_input":"2024-03-14T10:11:23.909565Z","iopub.status.idle":"2024-03-14T10:11:23.920447Z","shell.execute_reply.started":"2024-03-14T10:11:23.909534Z","shell.execute_reply":"2024-03-14T10:11:23.919034Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        GENRE                                        DESCRIPTION\n0      drama    Listening in to a conversation between his do...\n1   thriller    A brother and sister with a past incestuous r...\n2      adult    As the bus empties the students for their fie...\n3      drama    To help their unemployed father make ends mee...\n4      drama    The film's title refers not only to the un-re...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>GENRE</th>\n      <th>DESCRIPTION</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>drama</td>\n      <td>Listening in to a conversation between his do...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>thriller</td>\n      <td>A brother and sister with a past incestuous r...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>adult</td>\n      <td>As the bus empties the students for their fie...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>drama</td>\n      <td>To help their unemployed father make ends mee...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>drama</td>\n      <td>The film's title refers not only to the un-re...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"average_length = df['DESCRIPTION'].str.len().mean()\n\nprint(f\"The average length of movie descriptions is: {average_length:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:11:24.451235Z","iopub.execute_input":"2024-03-14T10:11:24.451589Z","iopub.status.idle":"2024-03-14T10:11:24.485995Z","shell.execute_reply.started":"2024-03-14T10:11:24.451561Z","shell.execute_reply":"2024-03-14T10:11:24.484949Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"The average length of movie descriptions is: 600.45\n","output_type":"stream"}]},{"cell_type":"code","source":"X=df['DESCRIPTION']\ny=df['GENRE']","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:12:41.223026Z","iopub.execute_input":"2024-03-14T10:12:41.223536Z","iopub.status.idle":"2024-03-14T10:12:41.228994Z","shell.execute_reply.started":"2024-03-14T10:12:41.223482Z","shell.execute_reply":"2024-03-14T10:12:41.227921Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:12:44.329296Z","iopub.execute_input":"2024-03-14T10:12:44.329682Z","iopub.status.idle":"2024-03-14T10:12:44.349119Z","shell.execute_reply.started":"2024-03-14T10:12:44.329656Z","shell.execute_reply":"2024-03-14T10:12:44.348289Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:13:01.678395Z","iopub.execute_input":"2024-03-14T10:13:01.678777Z","iopub.status.idle":"2024-03-14T10:13:01.687322Z","shell.execute_reply.started":"2024-03-14T10:13:01.678749Z","shell.execute_reply":"2024-03-14T10:13:01.685262Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import re\ndef clean_text(text):\n    # Remove special characters, punctuation, and symbols\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text\nX_train=X_train.apply(clean_text)\nX_test=X_test.apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:14:49.591512Z","iopub.execute_input":"2024-03-14T10:14:49.591909Z","iopub.status.idle":"2024-03-14T10:14:50.320893Z","shell.execute_reply.started":"2024-03-14T10:14:49.591878Z","shell.execute_reply":"2024-03-14T10:14:50.319308Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\ndef remove_stopwords(text):\n    stop_words = set(stopwords.words('english'))\n    new_text = [word for word in text.split() if word not in stop_words]\n    return ' '.join(new_text)\nX_train=X_train.apply(remove_stopwords)\nX_test=X_test.apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:15:22.560244Z","iopub.execute_input":"2024-03-14T10:15:22.560629Z","iopub.status.idle":"2024-03-14T10:15:29.632417Z","shell.execute_reply.started":"2024-03-14T10:15:22.560601Z","shell.execute_reply":"2024-03-14T10:15:29.631076Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import spacy \nnlp=spacy.load('en_core_web_sm')\nimport re \ndef tokenize(text):\n    tokens=re.split('\\W+',text)\n    return tokens\nX_train=X_train.apply(tokenize)\nX_test=X_test.apply(tokenize)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:15:48.531617Z","iopub.execute_input":"2024-03-14T10:15:48.531977Z","iopub.status.idle":"2024-03-14T10:15:53.469877Z","shell.execute_reply.started":"2024-03-14T10:15:48.531948Z","shell.execute_reply":"2024-03-14T10:15:53.468880Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\nps=PorterStemmer()\ndef steam_words(text):\n    return \" \".join([ps.stem(word) for word in text])\nX_train=X_train.apply(steam_words)\nX_test=X_test.apply(steam_words)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:16:14.748416Z","iopub.execute_input":"2024-03-14T10:16:14.749174Z","iopub.status.idle":"2024-03-14T10:17:34.932289Z","shell.execute_reply.started":"2024-03-14T10:16:14.749136Z","shell.execute_reply":"2024-03-14T10:17:34.930416Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:23:20.028500Z","iopub.execute_input":"2024-03-14T10:23:20.029295Z","iopub.status.idle":"2024-03-14T10:23:20.039069Z","shell.execute_reply.started":"2024-03-14T10:23:20.029253Z","shell.execute_reply":"2024-03-14T10:23:20.037539Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"19253    s e x   b e t r a y   s e d u c t   m a n i p ...\n12261    a u t i s t   t e e n   t o n i   f r i e n d ...\n37143    t h e   o n e h o u r   s p e c i a l   e x a ...\n21965    t h i   p e r s o n   n a r r   d o c u m e n ...\n7033     t h e   e a s y g o   v i l l a g   p o l i c ...\n                               ...                        \n11284    d a v i d   c r o n e n b e r g   w e l l k n ...\n44732    a d a n   n e u m a n n   m e m b e r   j e w ...\n38158    m i r a n d a   l e s b i a n   p h o t o g r ...\n860      t h e y   e n e m i   p r o f e s s i o n   l ...\n15795    I N   2 0 1 5   f a s h i o n   d e s i g n   ...\nName: DESCRIPTION, Length: 43371, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=100)  \nX_train_vectorized = vectorizer.fit_transform(X_train).toarray()\nprint(X_train_vectorized)\nX_test_vectorized = vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T10:21:42.681563Z","iopub.execute_input":"2024-03-14T10:21:42.681957Z","iopub.status.idle":"2024-03-14T10:21:44.322542Z","shell.execute_reply.started":"2024-03-14T10:21:42.681931Z","shell.execute_reply":"2024-03-14T10:21:44.321394Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      3\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \n\u001b[0;32m----> 4\u001b[0m X_train_vectorized \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train_vectorized)\n\u001b[1;32m      6\u001b[0m X_test_vectorized \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2128\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2129\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2130\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2131\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2132\u001b[0m )\n\u001b[0;32m-> 2133\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2135\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1296\u001b[0m         )\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n","\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"],"ename":"ValueError","evalue":"empty vocabulary; perhaps the documents only contain stop words","output_type":"error"}]},{"cell_type":"code","source":"print(tfidf)\nprint(tfidf.get_feature_names_out())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['DESCRIPTION_1']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport pandas as pd\nimport re\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The shpae of X_train is {X_train.shape}\")\nprint(f\"The shpae of X_test is {X_test.shape}\")\nprint(f\"The shpae of Y_train is {y_train.shape}\")\nprint(f\"The shpae of Y_test is {y_test.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=1000)  \nX_train_vectorized = vectorizer.fit_transform(X_train).toarray()\nprint(X_train_vectorized)\nX_test_vectorized = vectorizer.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert target labels to numerical values\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_vectorized.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_vectorized","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train_vectorized_ordered = tf.sparse.reorder(X_train_vectorized)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert them to SparseTensor\nX_train_sparse = tf.sparse.from_dense(X_train_vectorized)\nX_test_sparse = tf.sparse.from_dense(X_test_vectorized)\n\n# Now reorder the SparseTensors\nX_train_sparse = tf.sparse.reorder(X_train_sparse)\nX_test_sparse = tf.sparse.reorder(X_test_sparse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential([\n    Dense(512, activation='relu', input_dim=X_train_vectorized.shape[1]),\n    Dropout(0.8),\n    Dense(256, activation='relu'),\n    # Dropout(0.8),\n    Dense(128, activation='relu'),\n    # Dropout(0.8),\n\n    Dense(64, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(27, activation='softmax')\n])\n\n\n\noptimizer = Adam(learning_rate=0.003)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train_vectorized, y_train_encoded, epochs=15, batch_size=64, validation_data=(X_test_vectorized, y_test_encoded))\n# Evaluate the model on the testing dataset\ntest_loss, test_accuracy = model.evaluate(X_test_vectorized, y_test_encoded)\n\nprint(f'Test Accuracy: {test_accuracy}')\nprint(f'Test Loss: {test_loss}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}