{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzH3tn6wyb4Z",
        "outputId": "c2de1d52-fcb3-4022-cb38-a6bc9c2b1750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install wget\n",
        "!pip install torch -q\n",
        "!pip install transformers -q\n",
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  device_count = torch.cuda.device_count()\n",
        "  device_name = torch.cuda.get_device_name(0)\n",
        "\n",
        "  print(f\"There are {device_count} GPU(s) available.\")\n",
        "  print(f\"We will use the GPU: {device_name}\")\n",
        "\n",
        "\n",
        "else:\n",
        "  print(\"No GPU available, using the CPU instead.\")\n",
        "  device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKQ-7b2gylVB",
        "outputId": "206a4601-2bc4-4f9c-ff36-63f606486f2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "LoMjH9tPzDm-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PoemDataset(Dataset):\n",
        "    def __init__(self, sentences, poems, tokenizer, max_length):\n",
        "        self.sentences = sentences\n",
        "        self.poems = poems\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        poem = self.poems[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            poem,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': encoding['input_ids'].flatten()\n",
        "        }"
      ],
      "metadata": {
        "id": "YMaSSe6WzStR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_poem_dataset(angry_sentences, funny_poems, model_name='distilbert-base-uncased', max_length=128, batch_size=8):\n",
        "    # Ensure sentences and poems are paired correctly\n",
        "    assert len(angry_sentences) == len(funny_poems), \"Mismatch in number of sentences and poems\"\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_sentences, test_sentences, train_poems, test_poems = train_test_split(\n",
        "        angry_sentences, funny_poems, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = PoemDataset(train_sentences, train_poems, tokenizer, max_length)\n",
        "    test_dataset = PoemDataset(test_sentences, test_poems, tokenizer, max_length)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader, test_dataloader, tokenizer\n"
      ],
      "metadata": {
        "id": "_48If9cF3IBS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "angry_sentences = [\n",
        "    \"I can't believe they forgot my birthday!\",\n",
        "    \"This traffic is driving me crazy!\",\n",
        "    \"Why is the WiFi so slow today?\",\n",
        "    \"I'm so tired of eating the same thing every day!\",\n",
        "    \"My phone battery always dies when I need it most!\",\n",
        "    \"Why do I always lose my keys right when I'm late?\",\n",
        "    \"I hate it when people chew with their mouth open!\",\n",
        "    \"How come the line is always longest when I'm in a hurry?\",\n",
        "    \"Why does it always rain when I forget my umbrella?\",\n",
        "    \"I can't stand it when people don't use their turn signals!\"\n",
        "]\n",
        "\n",
        "funny_poems = [\n",
        "    \"Forgotten day, oh what a blight! / But who needs cake at midnight? / Perhaps they plan a grand surprise / Or simply can't read calendar's guise.\",\n",
        "    \"Cars crawl like snails on hot concrete / A turtle race can't be beat / In this jam, I'll grow a beard / Road rage? Nah, I'm just weird.\",\n",
        "    \"Internet crawls, my patience thins / Loading bar becomes my frenemy / I could've trained a pigeon / To deliver emails more speedy.\",\n",
        "    \"Monotonous meals, day after day / My taste buds threaten to run away / Perhaps I'll start a food rebellion / And eat my socks for this meal's hellion.\",\n",
        "    \"Battery drains, oh cruel device! / Always fails at moments precise / I'll invent a phone powered by sighs / Or just yell my messages to the skies.\",\n",
        "    \"Keys play hide and seek, what a game! / As I'm rushing out, they're to blame / I'll tie them to a giant balloon / So finding them won't spell my doom.\",\n",
        "    \"Open-mouthed chewers, please beware / Your dinner sounds pollute the air / I'll invent a mute button for mouths / Or dine exclusively down south.\",\n",
        "    \"Lines stretch long when time is tight / A cosmic joke, an endless plight / I'll master teleportation soon / Or just camp out since last June.\",\n",
        "    \"Raindrops fall as umbrellas hide / Weather forecasts have surely lied / I'll grow a waterproof hairdo / Or just pretend I'm at the zoo.\",\n",
        "    \"Turn signals forgotten, cars swerve / Testing each driver's last nerve / I'll invent telepathic cars / Or stick big arrows to their fars.\"\n",
        "]"
      ],
      "metadata": {
        "id": "aSSACDiB7OOG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, test_dataloader, tokenizer = prepare_poem_dataset(angry_sentences, funny_poems)\n",
        "\n",
        "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of test batches: {len(test_dataloader)}\")\n",
        "\n",
        "# Example of accessing a batch\n",
        "for batch in train_dataloader:\n",
        "    print(\"Input shape:\", batch['input_ids'].shape)\n",
        "    print(\"Attention mask shape:\", batch['attention_mask'].shape)\n",
        "    print(\"Labels shape:\", batch['labels'].shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkFntlHC7EBF",
        "outputId": "f89b8039-164d-47ef-a39e-696eb9a18d50"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training batches: 1\n",
            "Number of test batches: 1\n",
            "Input shape: torch.Size([8, 128])\n",
            "Attention mask shape: torch.Size([8, 128])\n",
            "Labels shape: torch.Size([8, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MdbCkfO22Sfs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}