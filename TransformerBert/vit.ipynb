{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vision Transformer: An Image is Worth 16X16 Words**\n",
    "\n",
    "- The Transformer architecture is based on self-attention mechanisms and is highly parallelizable. \n",
    "\n",
    "- Transformer processes data in a sequence-to-sequence manner, allowing it to capture long-range dependencies more effectively than traditional models like recurrent neural networks (RNNs).\n",
    "\n",
    "- Transformer architecture is initially designed for natural language processing tasks.\n",
    "\n",
    "- **Vision Transformers(ViTs) are an adaptation of the Transformer architecture to handle image data**. \n",
    "\n",
    "- [Paper](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "- Applications:\n",
    "   - Object detection \n",
    "   - Image segmentation\n",
    "   - Image classification\n",
    "   - Action recognition\n",
    "   - Visual grounding\n",
    "   - Visual-question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Adapting Transformer from NLP to Vision**\n",
    "* **From Words to Patches:**\n",
    "\n",
    "    * Transformers in NLP: \n",
    "\n",
    "        * In NLP, transformers process sentences by treating each word as an input token. \n",
    "        \n",
    "        * These tokens are then embedded into vectors and passed through the Transformer model to capture relationships between words.\n",
    "        \n",
    "    * Transformers in Vision: \n",
    "        * The core idea is to represent an **image as a sequence of smaller patches**\n",
    "\n",
    "        * Each patch is treated as a token. \n",
    "        \n",
    "        * These patches are embedded into vectors and fed into the Transformer model.\n",
    "\n",
    "*  **Self-Attention Mechanism**\n",
    "\n",
    "    * Global Context: \n",
    "    \n",
    "        * The self-attention mechanism in Transformers allows the model to consider the relationships between all tokens (patches in the case of ViTs) simultaneously.\n",
    "\n",
    "    * Attention Weights: \n",
    "\n",
    "        * Each patch's representation is updated by taking a weighted sum of the representations of all patches, where the weights are dynamically computed based on the similarity between patches. This allows the model to focus on the most relevant parts of the image for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoding Vision Transformer Architecture**\n",
    "<center>\n",
    "    <img src=\"./assets/vit.png\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Image Patching**\n",
    "* The input image is divided into fixed-size patches, typically 16x16 pixels. \n",
    "\n",
    "* Each patch is flattened into a 1D vector.\n",
    "\n",
    "* Consider an input image with dimensions 𝐻 × 𝑊 × 𝐶\n",
    "\n",
    "Let's say: Input Image→(224,224,3)\n",
    "\n",
    "The image is divided into smaller patches of size P×P pixels. Let's assume each patch is 16×16 pixels i.e. P=16\n",
    "\n",
    "Number of patches along height = 224/16 = 14\n",
    "\n",
    "Number of patches along width = 224/16 = 14\n",
    "\n",
    "Total patches =14×14=196\n",
    "\n",
    "Each 16×16 patch is flattened into a 1D vector. The dimension of this vector will be P×P×C=16×16×3=768\n",
    "\n",
    "Total 1D vectors of size 768 = 196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.  Patch Embedding**\n",
    "\n",
    "* After dividing the input image into patches and flattening them, each patch is a high-dimensional vector. \n",
    "\n",
    "* The next step is to project these high-dimensional vectors into a lower-dimensional embedding space using a learned embedding matrix.\n",
    "\n",
    "* In our example, this matrix will have dimensions (768,D), where D is the desired lower-dimensional space (embedding dimension).\n",
    "\n",
    "For example, if D=128:\n",
    "\n",
    "Embedding matrix E has dimensions (768,128)\n",
    "\n",
    "Resulting embedded patch vectors will have dimensions (196,128)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Positional Embedding**\n",
    " \n",
    "* To preserve the spatial information of the patches, positional embeddings are added to the patch embeddings. \n",
    "\n",
    "* These embeddings encode the position of each patch within the image.\n",
    "\n",
    "1. **Original Image**: $(224, 224, 3)$\n",
    "\n",
    "2. **Patch Size**: $16 \\times 16$\n",
    "\n",
    "3. **Number of Patches**: $14 \\times 14 = 196$\n",
    "\n",
    "4. **Flattened Patch Dimension**: $768$\n",
    "\n",
    "5. **Embedded Patch Dimension**: $128$\n",
    "\n",
    "6. **Resulting Embedded Patches $Z$**: $(196, 128)$\n",
    "\n",
    "7. **Positional Embedding Matrix Dimension $P$**: $(196, 128)$\n",
    "\n",
    "8. **Position Encoded Patches Embeddings**: $Z+P$ = $(196, 128)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Transformer Encoder**\n",
    "* The embedded patches are passed through a stack of transformer encoder layers.\n",
    "\n",
    "* Each layer consists of multi-head self-attention and feed-forward neural networks (FFNs). \n",
    "\n",
    "* The self-attention mechanism allows the model to capture dependencies between different patches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Classification Head**\n",
    "* The output of the transformer encoder is passed through a classification head, typically consisting of a layer normalization followed by a linear layer, to obtain the final class predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementing Step by Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Splitting and Patch Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the image into patches and then project them into a vector space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        \n",
    "        # Calculate the number of patches from the image size and patch size\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        \n",
    "        # Create a projection layer to convert the image into patches\n",
    "        # The layer projects each patch into a vector of size hidden_size\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        # self.projection = nn.Linear(self.num_patches*3, self.hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_channels, image_size, image_size) -> (batch_size, num_patches, hidden_size)(batchsize, 196, 128)\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 196, 128])\n"
     ]
    }
   ],
   "source": [
    "# cell 3\n",
    "# Testing the PatchEmbeddings class with dummy input\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 224,\n",
    "    \"patch_size\": 16,\n",
    "    \"num_channels\": 3,\n",
    "    \"hidden_size\": 128\n",
    "}\n",
    "\n",
    "\n",
    "patch_embeddings = PatchEmbeddings(config)\n",
    "\n",
    "input_tensor = torch.randn(8, 3, 224, 224)  # batch_size=8, num_channels=3, image_size=224\n",
    "output_tensor = patch_embeddings(input_tensor)\n",
    "\n",
    "print(output_tensor.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Position Encoded Embeddings**\n",
    "\n",
    "* **Note**: \n",
    "    * After the patches are converted to a sequence of embeddings, the [CLS] token is added to the beginning of the sequence, it will be used later in the classification layer to classify the image. \n",
    "    \n",
    "    * The [CLS] token’s embedding is learned during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 4\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine the patch embeddings with the class token and position embeddings.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        \n",
    "        # Create a learnable [CLS] token. [CLS] token is added to the beginning of the input sequence\n",
    "        # and is used to classify the entire sequence\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
    "        \n",
    "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
    "        # Add 1 to the sequence length for the [CLS] token\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches+1, config[\"hidden_size\"]))\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        # Expand the [CLS] token to the batch size\n",
    "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
    "        # This results in a sequence length of (num_patches + 1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Add the position embeddings to the input sequence\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 197, 128])\n"
     ]
    }
   ],
   "source": [
    "# cell 5\n",
    "\n",
    "# Testing the Position Encoded Embeddings with dummy input\n",
    "config = {\n",
    "    \"image_size\": 224,\n",
    "    \"patch_size\": 16,\n",
    "    \"num_channels\": 3,\n",
    "    \"hidden_size\": 128,\n",
    "    \"hidden_dropout_prob\": 0.5   \n",
    "}\n",
    "\n",
    "embedding = Embeddings(config)\n",
    "input_tensor = torch.randn(8, 3, 224, 224)  # batch_size=8, num_channels=3, image_size=224\n",
    "output_tensor = embedding(input_tensor)\n",
    "\n",
    "print(output_tensor.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single Attention Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 6\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head.\n",
    "    This module is used in the MultiHeadAttention module.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "        \n",
    "        # Create the query, key, and value projection layers\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Project the input into query, key, and value\n",
    "        # The same input is used to generate the query, key, and value,\n",
    "        # so it's usually called self-attention.\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, attention_head_size)\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        return (attention_output, attention_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output: torch.Size([8, 197, 32])\n",
      "Attention Probs: torch.Size([8, 197, 197])\n"
     ]
    }
   ],
   "source": [
    "# cell 7\n",
    "\n",
    "# Testing the AttentionHead class with dummy input\n",
    "hidden_size = 128\n",
    "attention_head_size = 32\n",
    "dropout = 0.5\n",
    "bias = True\n",
    "attention_head = AttentionHead(hidden_size, attention_head_size, dropout, bias)\n",
    "input_tensor = torch.randn(8, 197, 128)  # batch_size=8, sequence_length=197, hidden_size=128\n",
    "attention_output, attention_probs = attention_head(input_tensor)\n",
    "\n",
    "print(\"Attention Output:\", attention_output.shape)\n",
    "print(\"Attention Probs:\", attention_probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 8\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "    This module is used in the TransformerEncoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        \n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a list of attention heads\n",
    "        self.heads = nn.ModuleList([])\n",
    "        for _ in range(self.num_attention_heads):\n",
    "            head = AttentionHead(\n",
    "                self.hidden_size,\n",
    "                self.attention_head_size,\n",
    "                config[\"attention_probs_dropout_prob\"],\n",
    "                self.qkv_bias\n",
    "            )\n",
    "            self.heads.append(head)\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size, bias=True)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the attention output for each attention head\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        \n",
    "        # Concatenate the attention outputs from each attention head\n",
    "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
    "        \n",
    "        # Project the concatenated attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
    "            return (attention_output, attention_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output: torch.Size([8, 197, 128])\n"
     ]
    }
   ],
   "source": [
    "# cell 9\n",
    "\n",
    "# Testing MultiHeadAttention class with dummy input\n",
    "config = {\n",
    "    \"image_size\": 224,\n",
    "    \"patch_size\": 16,\n",
    "    \"num_channels\": 3,\n",
    "    \"hidden_size\": 128,\n",
    "    \"hidden_dropout_prob\": 0.5,\n",
    "    \"qkv_bias\": True,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"attention_probs_dropout_prob\": 0.5\n",
    "}\n",
    "multi_head_attention = MultiHeadAttention(config)\n",
    "input_tensor = torch.randn(8, 197, 128)  # batch_size=8, sequence_length=197, hidden_size=128\n",
    "attention_output, attention_probs = multi_head_attention(input_tensor)\n",
    "\n",
    "print(\"Attention Output:\", attention_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer Encoder**\n",
    "\n",
    "<center>\n",
    "    <img src=\"./assets/encoder.png\" width=200/>\n",
    "</center>\n",
    "\n",
    "* The transformer encoder is made of a stack of transformer layers. \n",
    "* Each transformer layer mainly consists of a multi-head attention module and a feed-forward network. \n",
    "* To better scale the model and stabilize training, two Layer normalization layers and skip connections are added to the transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 10\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer perceptron module.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.activation = nn.GELU()\n",
    "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 11\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Self-attention\n",
    "        attention_output, attention_probs = self.attention(self.layernorm_1(x), output_attentions=output_attentions)\n",
    "        \n",
    "        # Skip connection\n",
    "        x = x + attention_output\n",
    "        \n",
    "        # Feed-forward network\n",
    "        mlp_output = self.mlp(self.layernorm_2(x))\n",
    "        # Skip connection\n",
    "        x = x + mlp_output\n",
    "        # Return the transformer block's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, attention_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 12\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The transformer encoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Create a list of transformer blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_hidden_layers\"]):\n",
    "            block = Block(config)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the transformer block's output for each block\n",
    "        all_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x, attention_probs = block(x, output_attentions=output_attentions)\n",
    "            if output_attentions:\n",
    "                all_attentions.append(attention_probs)\n",
    "        # Return the encoder's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, all_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 13\n",
    "\n",
    "class ViTForClassfication(nn.Module):\n",
    "    \"\"\"\n",
    "    The ViT model for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        # Create the embedding module\n",
    "        self.embedding = Embeddings(config)\n",
    "        # Create the transformer encoder module\n",
    "        self.encoder = Encoder(config)\n",
    "        # Create a linear layer to project the encoder's output to the number of classes\n",
    "        self.classifier = nn.Linear(config[\"hidden_size\"], config[\"num_classes\"])\n",
    "        # Initialize the weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.position_embeddings.dtype)\n",
    "\n",
    "            module.cls_token.data = nn.init.trunc_normal_(\n",
    "                module.cls_token.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.cls_token.dtype)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the embedding output\n",
    "        embedding_output = self.embedding(x)\n",
    "        \n",
    "        # Calculate the encoder's output\n",
    "        encoder_output, all_attentions = self.encoder(embedding_output, output_attentions=output_attentions)\n",
    "        \n",
    "        # Calculate the logits, take the [CLS] token's output as features for classification\n",
    "        logits = self.classifier(encoder_output[:, 0])\n",
    "        # Return the logits and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (logits, None)\n",
    "        else:\n",
    "            return (logits, all_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 14\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def prepare_data(batch_size=4, num_workers=2, train_sample_size=None, test_sample_size=None):\n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=train_transform)\n",
    "    if train_sample_size is not None:\n",
    "        # Randomly sample a subset of the training set\n",
    "        indices = torch.randperm(len(trainset))[:train_sample_size]\n",
    "        trainset = torch.utils.data.Subset(trainset, indices)\n",
    "    \n",
    "\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    test_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=test_transform)\n",
    "    if test_sample_size is not None:\n",
    "        # Randomly sample a subset of the test set\n",
    "        indices = torch.randperm(len(testset))[:test_sample_size]\n",
    "        testset = torch.utils.data.Subset(testset, indices)\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    return trainloader, testloader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 15\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, optimizer, loss_fn, device):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, trainloader, testloader, epochs, save_model_every_n_epochs=0):\n",
    "        \"\"\"\n",
    "        Train the model for the specified number of epochs.\n",
    "        \"\"\"\n",
    "        # Keep track of the losses and accuracies\n",
    "        train_losses, test_losses, accuracies = [], [], []\n",
    "        # Train the model\n",
    "        for i in range(epochs):\n",
    "            train_loss = self.train_epoch(trainloader)\n",
    "            accuracy, test_loss = self.evaluate(testloader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            if save_model_every_n_epochs > 0 and (i+1) % 5 == 0 and i+1 != epochs:\n",
    "                print('\\tSave checkpoint at epoch', i+1)\n",
    "                torch.save(self.model.state_dict(), 'model_epoch_' + str(i+1) + '.pt')\n",
    "                \n",
    "        return train_losses, test_losses, accuracies\n",
    "\n",
    "    def train_epoch(self, trainloader):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for batch in trainloader:\n",
    "            # Move the batch to the device\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "            images, labels = batch\n",
    "            # Zero the gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            # Calculate the loss\n",
    "            loss = self.loss_fn(self.model(images)[0], labels)\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "            # Update the model's parameters\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item() * len(images)\n",
    "        return total_loss / len(trainloader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, testloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                # Move the batch to the device\n",
    "                batch = [t.to(self.device) for t in batch]\n",
    "                images, labels = batch\n",
    "                \n",
    "                # Get predictions\n",
    "                logits, _ = self.model(images)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "                total_loss += loss.item() * len(images)\n",
    "\n",
    "                # Calculate the accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "        accuracy = correct / len(testloader.dataset)\n",
    "        avg_loss = total_loss / len(testloader.dataset)\n",
    "        return accuracy, avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:33<00:00, 5019702.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Cell 16\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "trainloader, testloader, _ = prepare_data(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fm-pc-lt-238/miniconda3/envs/soccer-analytics/lib/python3.9/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.8559, Test loss: 1.6994, Accuracy: 0.3842\n",
      "Epoch: 2, Train loss: 1.5523, Test loss: 1.4242, Accuracy: 0.4797\n",
      "Epoch: 3, Train loss: 1.4072, Test loss: 1.3158, Accuracy: 0.5237\n"
     ]
    }
   ],
   "source": [
    "# Cell 17\n",
    "\n",
    "config = {\n",
    "    \"patch_size\": 4,  # Input image size: 32x32 -> 8x8 patches\n",
    "    \"hidden_size\": 48,\n",
    "    \"num_hidden_layers\": 4,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"intermediate_size\": 4 * 48, # 4 * hidden_size\n",
    "    \"hidden_dropout_prob\": 0.0,\n",
    "    \"attention_probs_dropout_prob\": 0.0,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"image_size\": 32,\n",
    "    \"num_classes\": 10, # num_classes of CIFAR10\n",
    "    \"num_channels\": 3,\n",
    "    \"qkv_bias\": True,\n",
    "    \"use_faster_attention\": True,\n",
    "}\n",
    "epochs = 3\n",
    "lr = 0.001\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Create the model, optimizer, loss function and trainer\n",
    "model = ViTForClassfication(config)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "trainer = Trainer(model, optimizer, loss_fn, device=device)\n",
    "train_losses, test_losses, accuracies = trainer.train(trainloader, testloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soccer-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
