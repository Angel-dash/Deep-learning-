{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8Z_hPizo9hK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring Sentence-,\n",
        "Document-, and Character Level Embeddings"
      ],
      "metadata": {
        "id": "PGoz4z4kpHdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "information related to the ordering of words, along with their semantics, can be taken into\n",
        "account when building embeddings to represent words.  \n",
        "Doc2vec which will provide an embedding for entire documents."
      ],
      "metadata": {
        "id": "4jUNuIeJpnHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In Learn\n",
        "Natural Language Processing (NLP), along with the word vectors of Learn,\n",
        "Natural, and Language, the Document vector is used to predict the next\n",
        "word, Processing. The model is tuned based on how it did in terms of predicting\n",
        "the word Processing and how it learned throughout"
      ],
      "metadata": {
        "id": "fBL9ByYlrOwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributed Bag-of-Words Model of Paragraph Vectors (PV-DBOW): In this\n",
        "approach, word vectors aren't taken into account. Instead, the paragraph vector\n",
        "is used to predict randomly sampled words from the paragraph. In the process of\n",
        "using gradient descent and backpropagation, the paragraph vectors get adjusted\n",
        "and learning happens based on how good or bad they are doing in terms of\n",
        "making predictions. This approach is analogous to the Skip-gram approach used\n",
        "in Word2Vec"
      ],
      "metadata": {
        "id": "ks-F_3VArdGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building a Doc2Vec model**\n"
      ],
      "metadata": {
        "id": "sUeD6iqQrtnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ],
      "metadata": {
        "id": "ddTAUMaspIVn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny4UHwRJryzC",
        "outputId": "706eb2a7-fdab-4838-8be5-5e79d27e304b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human', 'interface', 'computer'],\n",
              " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
              " ['eps', 'user', 'interface', 'system'],\n",
              " ['system', 'human', 'system', 'eps'],\n",
              " ['user', 'response', 'time'],\n",
              " ['trees'],\n",
              " ['graph', 'trees'],\n",
              " ['graph', 'minors', 'trees'],\n",
              " ['graph', 'minors', 'survey']]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents=[TaggedDocument(doc,[i]) for i,doc in enumerate(common_texts)]\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEqer4Avr09g",
        "outputId": "130e65cf-847f-4b0c-e307-0de3b80fe401"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['human', 'interface', 'computer'], tags=[0]),\n",
              " TaggedDocument(words=['survey', 'user', 'computer', 'system', 'response', 'time'], tags=[1]),\n",
              " TaggedDocument(words=['eps', 'user', 'interface', 'system'], tags=[2]),\n",
              " TaggedDocument(words=['system', 'human', 'system', 'eps'], tags=[3]),\n",
              " TaggedDocument(words=['user', 'response', 'time'], tags=[4]),\n",
              " TaggedDocument(words=['trees'], tags=[5]),\n",
              " TaggedDocument(words=['graph', 'trees'], tags=[6]),\n",
              " TaggedDocument(words=['graph', 'minors', 'trees'], tags=[7]),\n",
              " TaggedDocument(words=['graph', 'minors', 'survey'], tags=[8])]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Doc2Vec(documents, vector_size=5, min_count=1, workers=4,epochs = 40)\n",
        "model.train(documents, total_examples=model.corpus_count,\n",
        "epochs=model.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-2EZcAzsM_u",
        "outputId": "a58b6732-b006-4658-ed31-ba2a778c3efe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.vector_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7pfP9h6s3Hh",
        "outputId": "4e4cb043-80a1-4485-91c7-6eb14571c725"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.docvecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pU1eJaLs9xk",
        "outputId": "c39fb464-0bc0-4009-8e5f-05e48337e5cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-c8c8dc9618a3>:1: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
            "  len(model.docvecs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.wv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OokyLBVctCxE",
        "outputId": "e66a3b7f-4f5f-450b-efe8-1df78aff209c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(model.wv.key_to_index.keys())\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEeicdIhtIQH",
        "outputId": "7e3ff806-8707-4984-e3c9-96587a28d006"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['system',\n",
              " 'graph',\n",
              " 'trees',\n",
              " 'user',\n",
              " 'minors',\n",
              " 'eps',\n",
              " 'time',\n",
              " 'response',\n",
              " 'survey',\n",
              " 'computer',\n",
              " 'interface',\n",
              " 'human']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector=model.infer_vector(['user', 'interface', 'for','computer'])\n",
        "vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3z-f2oKtY2g",
        "outputId": "c38d0505-9a19-490f-fb0f-56cd37a6a089"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.037988  ,  0.07104281,  0.07832859, -0.04058238, -0.02786596],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing vector size and min_count"
      ],
      "metadata": {
        "id": "Z02l_G6dvNTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Doc2Vec(documents, min_count=3,epochs=40,vector_size=50)\n",
        "model.train(documents, total_examples=model.corpus_count,epochs=model.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFiZ-qttufyg",
        "outputId": "fc067427-7516-49fb-daaf-fbc1ebec8a9e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.wv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7wrTizNvptc",
        "outputId": "37cac609-ea7b-4dff-999f-2e5ef0214b6f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word=list(model.wv.key_to_index.keys())\n",
        "word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGUL6r-tv_D-",
        "outputId": "10fa7ead-fe7d-4945-c582-5a65947794ed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['system', 'graph', 'trees', 'user']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector=model.infer_vector(['user', 'interface', 'for','computer'])\n",
        "vector\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n15DZ9smwJhQ",
        "outputId": "afecb082-350f-472f-c667-c7576bd0124e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.5629326e-03,  7.3750918e-03,  8.4501738e-03, -3.5475469e-03,\n",
              "       -2.8910767e-03, -2.3498680e-03, -8.2272002e-03,  9.4008390e-03,\n",
              "       -1.9002014e-03, -2.6745542e-03, -9.1982633e-03,  5.4919636e-03,\n",
              "       -9.3357754e-04, -6.2426929e-03, -3.1859947e-03,  9.0781618e-03,\n",
              "       -5.1593612e-04, -7.4274004e-03, -7.8136002e-04, -3.6041581e-03,\n",
              "       -5.8137430e-03,  7.7605299e-03, -9.0763299e-03, -5.2079242e-03,\n",
              "        3.5809651e-03, -8.5429680e-03, -5.2760406e-03,  7.4751739e-04,\n",
              "       -2.4859058e-03,  2.5844565e-04, -3.6902253e-03,  6.6739880e-03,\n",
              "       -4.7837901e-03, -5.7963510e-03,  6.1678854e-03, -5.3438535e-03,\n",
              "        3.2097080e-03, -1.3939353e-03, -2.3624424e-03,  6.7345849e-05,\n",
              "        1.9947926e-03,  8.4804632e-03,  4.6184566e-03,  9.4201816e-03,\n",
              "        5.5492385e-03, -3.0637481e-03,  7.9879649e-03,  8.4852660e-03,\n",
              "       -8.2042739e-03, -8.4018605e-03], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the vector size is now 50 and only 4 terms are in the vocabulary.\n",
        "This is because min_count was modified to 3 and, consequently, terms that were\n",
        "equal to or greater than 3 terms are present in the vocabulary now."
      ],
      "metadata": {
        "id": "I-zyKAFgwbpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing two different approaches of doc2vec\n",
        "\n",
        "\n",
        "1.   PV-DM\n",
        "2.   PV-DBOW\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYvBM1lnwki7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Doc2Vec(documents,vector_size=50,min_count=2,epochs=40,dm=1)\n",
        "model.train(documents,total_examples=model.corpus_count,epochs=model.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9tDJEh8wVd2",
        "outputId": "f2cf53af-f5c6-4d37-ed1a-ae2c7d512d70"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dm equal to 0 builds the Doc2Vec model based on the distributed bag-of-words approach and vice versa"
      ],
      "metadata": {
        "id": "D7w2wSD0xMJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Doc2Vec(documents,vector_size=50,min_count=2,epochs=40,dm=0)\n",
        "model.train(documents, total_examples=model.corpus_count,epochs=model.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fM67JhsxG-g",
        "outputId": "94c1a873-1a52-4a0e-8177-268161e8df05"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dm_concat parameter is used in the PV-DM approach. Its value, when set to 1,\n",
        "indicates to the algorithm that the context vectors should be concatenated while trying to\n",
        "predict the target word. This, of course, leads to building a larger model since multiple\n",
        "word embeddings get concatenated."
      ],
      "metadata": {
        "id": "L3_L4q9HxuWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Doc2Vec(documents,vector_size=50,min_count=2,epochs=40,dm=1,window=2,min_alpha=0.005,dm_concat=1)\n",
        "model.train(documents, total_examples=model.corpus_count,epochs=model.epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0mhuZLXxjmK",
        "outputId": "a5afbc04-3410-429a-b3ff-3fd40ed5728a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The window size parameter controls the distance between the word under concentration\n",
        "and the word to be predicted, similar to the Word2Vec approach."
      ],
      "metadata": {
        "id": "88hGMJAiZhMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Doc2Vec(documents,vector_size=50,min_count=2,epochs=40,window=2,dm=0)\n",
        "model.train(documents,total_examples=model.corpus_count,epochs=model.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s17KfNuxyJA0",
        "outputId": "082c5f65-101e-4b9a-b825-e44c14ec7b18"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's explore what the learning rate is and how it can be leveraged."
      ],
      "metadata": {
        "id": "P9qsO_n3Z_K9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " For Doc2Vec,\n",
        "the initial learning rate can be specified using the alpha parameter. With the min_alpha\n",
        "parameter, we can specify what value the learning rate should drop to over the course of\n",
        "training"
      ],
      "metadata": {
        "id": "og0g5JDQaSr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Doc2Vec(documents, vector_size=50,min_count=2,epochs=40,alpha=0.3,min_alpha=0.05,window=2,dm=1)\n",
        "model.train(documents, epochs=model.epochs,total_examples=model.corpus_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yauFO3emZ7PU",
        "outputId": "af223bc8-ae5a-45e9-c1fa-71fbbe20fc84"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uUqXty7aat6E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}