{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OLkYha2766X"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "X = (\"Computers can analyze text\",\n",
        " \"They do it using vectors and matrices\",\n",
        " \"Computers can process massive amounts of text data\")\n",
        "vectorizer=CountVectorizer(stop_words='english')\n",
        "X_vectorized=vectorizer.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.vocabulary_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_E-pTN98MRY",
        "outputId": "95245c64-dcdc-4541-cfbb-e4c97e15182e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'computers': 2, 'analyze': 1, 'text': 7, 'using': 8, 'vectors': 9, 'matrices': 5, 'process': 6, 'massive': 4, 'amounts': 0, 'data': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_vectorized.todense())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I9vJ6aT8mPb",
        "outputId": "aa7253de-e3d4-4677-a0b5-03d445befcfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0 0 1 1]\n",
            " [1 0 1 1 1 0 1 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring the bag of words architecture"
      ],
      "metadata": {
        "id": "4mLJ4-ux9FwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "P3BkxfE38suj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1813c5-d9a5-43cb-f5ca-1e166ef5e848"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"We are reading about Natural Language Processing Here\",\n",
        "\"Natural Language Processing making computers comprehend language data\",\n",
        "\"The field of Natural Language Processing is evolving everyday\"]"
      ],
      "metadata": {
        "id": "KsB90dFk_NAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=pd.Series(sentences)\n",
        "corpus\n"
      ],
      "metadata": {
        "id": "bDJtr8Nt_U3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b0efa0-99cc-4931-9b7a-cd39f705884c"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    We are reading about Natural Language Processi...\n",
              "1    Natural Language Processing making computers c...\n",
              "2    The field of Natural Language Processing is ev...\n",
              "dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "    '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "\n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "\n",
        "    Output : Returns the cleaned text corpus\n",
        "\n",
        "    '''\n",
        "    cleaned_corpus = pd.Series()\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            if word not in keep_list:\n",
        "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "                p1 = p1.lower()\n",
        "                qs.append(p1)\n",
        "            else : qs.append(word)\n",
        "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
        "    return cleaned_corpus"
      ],
      "metadata": {
        "id": "FxG1cVrP_kCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_removal(corpus):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "5JSH3N4jAX22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "WA7otvGsAtqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type=='snowball':\n",
        "      stemmer=SnowballStemmer(lanaguage='english')\n",
        "      corpus=[[stemmer.stem(x) for x in x ] for x in corpus]\n",
        "    else:\n",
        "      stemmer=PorterStemmer()\n",
        "      corpus=[[stemmer.stem(x) for x in x] for x in corpus]\n"
      ],
      "metadata": {
        "id": "ucKW9CpIA3AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "\n",
        "    Input :\n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should\n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "\n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "\n",
        "    Output : Returns the processed text corpus\n",
        "\n",
        "    '''\n",
        "\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus, keep_list)\n",
        "\n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "\n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "\n",
        "\n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "-b8fqm30A7He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_dot_words = ['U.S.', 'Mr.', 'Mrs.', 'D.C.']"
      ],
      "metadata": {
        "id": "80BviWVQA_I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing with Lemmatization here\n",
        "preprocessed_corpus = preprocess(corpus, keep_list = common_dot_words, stemming = False, stem_type = None,\n",
        "                                lemmatization = True, remove_stopwords = True)\n",
        "preprocessed_corpus"
      ],
      "metadata": {
        "id": "-5txHUG7BCFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7e83ad-6d44-437b-92d7-b4e1041bda88"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-f8a15d8180b8>:11: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  cleaned_corpus = pd.Series()\n",
            "<ipython-input-7-f8a15d8180b8>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-7-f8a15d8180b8>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-7-f8a15d8180b8>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['read natural language process',\n",
              " 'natural language process make computers comprehend language data',\n",
              " 'field natural language process evolve everyday']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the vocabulary\n"
      ],
      "metadata": {
        "id": "I8HHwlGtBM4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_of_words = set()\n",
        "for sentence in preprocessed_corpus:\n",
        "    for word in sentence.split():\n",
        "        set_of_words.add(word)\n",
        "vocab = list(set_of_words)\n",
        "print(vocab)"
      ],
      "metadata": {
        "id": "9f4Cbsz0BG5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7a1359-8301-4939-8bda-a1f3883f09ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['language', 'natural', 'computers', 'read', 'process', 'everyday', 'field', 'evolve', 'comprehend', 'data', 'make']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position = {}\n",
        "for i, token in enumerate(vocab):\n",
        "    position[token] = i\n",
        "print(position)"
      ],
      "metadata": {
        "id": "er4cK338BPM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc3b983-53c1-4a11-cd2d-232483659cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'language': 0, 'natural': 1, 'computers': 2, 'read': 3, 'process': 4, 'everyday': 5, 'field': 6, 'evolve': 7, 'comprehend': 8, 'data': 9, 'make': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a bow matrix"
      ],
      "metadata": {
        "id": "_4TKsrKQBUdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix = np.zeros((len(preprocessed_corpus), len(vocab)))"
      ],
      "metadata": {
        "id": "CzUeuUqQBRpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, preprocessed_sentence in enumerate(preprocessed_corpus):\n",
        "    for token in preprocessed_sentence.split():\n",
        "        bow_matrix[i][position[token]] = bow_matrix[i][position[token]] + 1"
      ],
      "metadata": {
        "id": "wh6ff-i0Ba36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix"
      ],
      "metadata": {
        "id": "-53ogLPjBde9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d01f5c6-3e83-4a72-9805-bef7b7948265"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [2., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1.],\n",
              "       [1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Countvectorizer for all the above process"
      ],
      "metadata": {
        "id": "2B9Kwah-KBwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=CountVectorizer()\n",
        "bow_matrix=vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "1Tk3miP5BgFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix"
      ],
      "metadata": {
        "id": "cBfSM6R_KNiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9025517d-89f6-45a6-86c3-8f22d2d8045c"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<3x11 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 17 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__v4pnXaKgNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "bow_matrix.toarray()\n"
      ],
      "metadata": {
        "id": "-rQnTkeVKkjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd2d257-c05e-429c-bb8e-fe7ce589783f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1],\n",
              "       [1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 0],\n",
              "       [0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_ngram_range = CountVectorizer(analyzer='word',ngram_range=(1,3))\n",
        "bow_matrix_ngram =vectorizer_ngram_range.fit_transform(preprocessed_corpus)\n",
        "print(vectorizer_ngram_range.get_feature_names_out())\n",
        "print(bow_matrix_ngram.toarray())"
      ],
      "metadata": {
        "id": "AfMoa2xyKu3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ab6e3c-d8b4-49bd-fe9a-a0281fb25cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['comprehend' 'comprehend language' 'comprehend language data' 'computers'\n",
            " 'computers comprehend' 'computers comprehend language' 'data' 'everyday'\n",
            " 'evolve' 'evolve everyday' 'field' 'field natural'\n",
            " 'field natural language' 'language' 'language data' 'language process'\n",
            " 'language process evolve' 'language process make' 'make' 'make computers'\n",
            " 'make computers comprehend' 'natural' 'natural language'\n",
            " 'natural language process' 'process' 'process evolve'\n",
            " 'process evolve everyday' 'process make' 'process make computers' 'read'\n",
            " 'read natural' 'read natural language']\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1]\n",
            " [1 1 1 1 1 1 1 0 0 0 0 0 0 2 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Max features in countvectorizer helps us to keep a cap on the number of features that can be used. Keep in mind that increasing the number of dimension can lead to overfitting which is also known as the curse of dimensionality"
      ],
      "metadata": {
        "id": "IBw1Cs10LwiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_ngram_range = CountVectorizer(analyzer='word',ngram_range=(1,3),max_features=6)\n",
        "bow_matrix_ngram =vectorizer_ngram_range.fit_transform(preprocessed_corpus)\n",
        "print(vectorizer_ngram_range.get_feature_names_out())\n",
        "print(bow_matrix_ngram.toarray())"
      ],
      "metadata": {
        "id": "--lYcL66LbWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d4a6a25-9db2-4a7e-b9f9-cf9b0d130e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['language' 'language process' 'natural' 'natural language'\n",
            " 'natural language process' 'process']\n",
            "[[1 1 1 1 1 1]\n",
            " [2 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using max_df and min_fdf which will ignore the number of words whose frequency is higher then the max_df as well has ignore the words that occurs in lesser amount then min_df\n"
      ],
      "metadata": {
        "id": "vgWHCjXIMnOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Issue with the bag of words approach\n",
        "\n",
        "\n",
        "*   OOV problem\n",
        "*   Cannot capture the semantic meaning behind the sentences\n",
        "\n"
      ],
      "metadata": {
        "id": "WpSWxznDNOoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KUKMSsdyNviz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)\n"
      ],
      "metadata": {
        "id": "vDBe4s2sMJ50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(tf_idf_matrix.toarray())\n",
        "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)\n"
      ],
      "metadata": {
        "id": "yVN2qa5xOh6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ebc2641-6ca1-4f05-91ff-e7d53b7c6109"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n",
            " [0.40512186 0.40512186 0.40512186 0.         0.         0.\n",
            "  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n",
            " [0.         0.         0.         0.49711994 0.49711994 0.49711994\n",
            "  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n",
            "\n",
            "The shape of the TF-IDF matrix is:  (3, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above code demonstrate the affect using L1 norm and L2 norm. The sum of all the mean absolute value of the vectors for that particular document should be 1 likewise the sum of all the means square value of the vector for the document should be 1 in L2 norm. By default tfidf uses L2 norm. While there is no difference in the vectorization process the value assigned to those tokens would be different for those two methods"
      ],
      "metadata": {
        "id": "UIqeKaE9DAe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While tf-idf is computationally fast and provides a way to assign weights to the words that are few amount of times it still fails to address following issues:\n",
        "\n",
        "\n",
        "*   Slow for large vocabulary\n",
        "*   cannot represent the semantics meaning of the sentences\n",
        "\n"
      ],
      "metadata": {
        "id": "q7ERBez_Dyq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cosine similarity**\n",
        "If two words have similar meaning then their magnitude and direction shoulde be similar. We can calculate consine similarity which gives values from -1 to +1\n",
        "+1 indicates that the varibales are prefectly similar and -1 indicates they are completely opposite."
      ],
      "metadata": {
        "id": "HlxLS03rEa2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vector1, vector2):\n",
        "  vector1=np.array(vector1)\n",
        "  vector2=np.array(vector2)\n",
        "  return np.dot(vector1, vector2) / (np.sqrt(np.sum(vector1**2)) * np.sqrt(np.sum(vector2**2)))\n"
      ],
      "metadata": {
        "id": "KVOjIrkKRXP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "giN9xnJOGJQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "id": "vBdnlA4QGQ6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7258251-a3e0-4156-91be-f18ff6bbac68"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0 0 0 0 0 0 1 0 1 1 1]\n",
            " [1 1 1 0 0 0 2 1 1 1 0]\n",
            " [0 0 0 1 1 1 1 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(bow_matrix.shape[0]):\n",
        "    for j in range(i + 1, bow_matrix.shape[0]):\n",
        "        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n",
        "              cosine_similarity(bow_matrix.toarray()[i], bow_matrix.toarray()[j]))"
      ],
      "metadata": {
        "id": "JZ20_ZtEGlHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c9a7b4-5919-467c-9e38-6796f3b63af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cosine similarity between the documents  0 and 1 is:  0.6324555320336759\n",
            "The cosine similarity between the documents  0 and 2 is:  0.6123724356957946\n",
            "The cosine similarity between the documents  1 and 2 is:  0.5163977794943223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One hot vectors\n",
        "Represents categorical values into numeric values. Assigns 1 to the position of the token and zero elsewherer"
      ],
      "metadata": {
        "id": "Qlec3O7dLAsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = [\"We are reading about Natural Language Processing Here\"]\n",
        "corpus2=pd.Series(sentence)\n",
        "corpus2"
      ],
      "metadata": {
        "id": "wwPg6_aeJVfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1f33ff-69cb-4540-f37a-fb1694ce0957"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    We are reading about Natural Language Processi...\n",
              "dtype: object"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing with Lemmatization here\n",
        "preprocessed_corpus = preprocess(corpus, keep_list = [], stemming =False, stem_type = None,lemmatization = True, remove_stopwords =True)\n",
        "preprocessed_corpus\n"
      ],
      "metadata": {
        "id": "COdF4giyMUEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8aeb99a-bb90-4888-bdc9-40b48f55ca6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-f8a15d8180b8>:11: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  cleaned_corpus = pd.Series()\n",
            "<ipython-input-7-f8a15d8180b8>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-7-f8a15d8180b8>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-7-f8a15d8180b8>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['read natural language process',\n",
              " 'natural language process make computers comprehend language data',\n",
              " 'field natural language process evolve everyday']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_of_words = set()\n",
        "for word in preprocessed_corpus[0].split():\n",
        "    set_of_words.add(word)\n",
        "vocab = list(set_of_words)\n",
        "print(vocab)"
      ],
      "metadata": {
        "id": "KwLXln62MoSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23afcec-f6d1-44d7-9719-fc1a02aa3b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['process', 'read', 'language', 'natural']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_corpus"
      ],
      "metadata": {
        "id": "3_2sFG8hNENp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fbdb3ee-91f2-4e22-aba9-1b900af753e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['read natural language process',\n",
              " 'natural language process make computers comprehend language data',\n",
              " 'field natural language process evolve everyday']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_corpus[0]"
      ],
      "metadata": {
        "id": "X-IYKbRXNu7j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a387083b-5f0f-427a-b3e3-9e1008140eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'read natural language process'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position = {}\n",
        "for i, token in enumerate(vocab):\n",
        "    position[token] = i\n",
        "print(position)"
      ],
      "metadata": {
        "id": "d6i4M_CAN2Uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d373ca6b-8a20-4495-e3e1-f71b044ab667"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'process': 0, 'read': 1, 'language': 2, 'natural': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_matrix = np.zeros((len(preprocessed_corpus[0].split()), len(vocab)))\n",
        "one_hot_matrix.shape"
      ],
      "metadata": {
        "id": "kvnaCdu-OFrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cea7abe-21ba-4360-f7ac-08ce7a19c6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 4)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_matrix"
      ],
      "metadata": {
        "id": "uKWzwpMnORpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5342ffc-99f9-47f9-bb14-b4400babccfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, token in enumerate(preprocessed_corpus[0].split()):\n",
        "    one_hot_matrix[i][position[token]] = 1\n",
        "    print(position[token])\n"
      ],
      "metadata": {
        "id": "_03Tq2-_OZmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94019eeb-dbb8-4455-a115-a8530bbb69f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "3\n",
            "2\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_matrix"
      ],
      "metadata": {
        "id": "rA0J6oW_QAer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b413f6-ab86-4d4b-e329-ec3a4901a2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0.]])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a basic chatbot"
      ],
      "metadata": {
        "id": "5O-5Sa_jbfBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "id": "6DJVktvCUI6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdc96626-6390-4412-f0b4-ca688e89c812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/MiniProjectdata/QA.zip"
      ],
      "metadata": {
        "id": "nxFw1BQNUaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84149f76-8a79-4405-b929-fbec74f3532b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/MiniProjectdata/QA.zip\n",
            "replace qa_Electronics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: qa_Electronics.json     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading a json files and extracting the questions and answers\n"
      ],
      "metadata": {
        "id": "IFr6UIE1Wjv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "#loading questions and answers in separate lists\n",
        "import ast\n",
        "questions = []\n",
        "answers = []\n",
        "with open('qa_Electronics.json','r') as f:\n",
        "    for line in f:\n",
        "        data = ast.literal_eval(line)\n",
        "        questions.append(data['question'].lower())\n",
        "        answers.append(data['answer'].lower())"
      ],
      "metadata": {
        "id": "_cwl41MpWB3m"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_vec = vectorizer.fit_transform(questions)\n",
        "tfidf = TfidfTransformer() #by default applies \"l2\" normalization\n",
        "X_tfidf = tfidf.fit_transform(X_vec)"
      ],
      "metadata": {
        "id": "tJ3AswTxWLfR"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to caculate teh consine similarity between teh tfidf vectors each row with the new questions and if the new questions and the vectorized question on teh tfidf vecotr then we can provide a answer and if the questions similarity are very low, lower then the certain threshold then we will descard the questions"
      ],
      "metadata": {
        "id": "n1kpO2aKYQqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conversation(im):\n",
        "    global tfidf, answers, X_tfidf\n",
        "    Y_vec = vectorizer.transform(im)\n",
        "    Y_tfidf = tfidf.fit_transform(Y_vec)\n",
        "    cos_sim = np.rad2deg(np.arccos(max(cosine_similarity(Y_tfidf, X_tfidf)[0])))\n",
        "    if cos_sim > 60 :\n",
        "        return \"sorry, I did not quite understand that\"\n",
        "    else:\n",
        "        return answers[np.argmax(cosine_similarity(Y_tfidf, X_tfidf)[0])]\n",
        "\n"
      ],
      "metadata": {
        "id": "YwW31CAPXA10"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the above consine_similarity return the array or spare representation of array\n",
        "Argmax retrivese the index that contanins the maximum values"
      ],
      "metadata": {
        "id": "FRALxUkxbHV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    usr = input(\"Please enter your username: \")\n",
        "    print(\"support: Hi, welcome to Q&A support. How can I help you?\")\n",
        "    while True:\n",
        "        im = input(\"{}: \".format(usr))\n",
        "        if im.lower() == 'bye':\n",
        "            print(\"Q&A support: bye!\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Q&A support: \"+conversation([im]))"
      ],
      "metadata": {
        "id": "UmwmoG-nW57i"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q_JUfFocO-h",
        "outputId": "33d729aa-f311-41bd-e017-7761f2b9254a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your username: Angel\n",
            "support: Hi, welcome to Q&A support. How can I help you?\n",
            "Angel: what's my phone battery\n",
            "Q&A support: this battery is intended for backup of the phone.\n",
            "Angel: Does it have bluetooth?\n",
            "Q&A support: no\n",
            "Angel: bye\n",
            "Q&A support: bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "srvYr4z5cjmt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}