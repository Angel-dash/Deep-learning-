{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OLkYha2766X"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "X = (\"Computers can analyze text\",\n",
        " \"They do it using vectors and matrices\",\n",
        " \"Computers can process massive amounts of text data\")\n",
        "vectorizer=CountVectorizer(stop_words='english')\n",
        "X_vectorized=vectorizer.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.vocabulary_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_E-pTN98MRY",
        "outputId": "d122e74d-0727-4179-e24a-c89b21c46340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'computers': 2, 'analyze': 1, 'text': 7, 'using': 8, 'vectors': 9, 'matrices': 5, 'process': 6, 'massive': 4, 'amounts': 0, 'data': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_vectorized.todense())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I9vJ6aT8mPb",
        "outputId": "dbbcd73a-6a75-4371-f1a4-ea119b9d3857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0 0 1 1]\n",
            " [1 0 1 1 1 0 1 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring the bag of words architecture"
      ],
      "metadata": {
        "id": "4mLJ4-ux9FwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3BkxfE38suj",
        "outputId": "a4f9b769-c77c-460e-e8e5-cf3b427eff96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"We are reading about Natural Language Processing Here\",\n",
        "\"Natural Language Processing making computers comprehend language data\",\n",
        "\"The field of Natural Language Processing is evolving everyday\"]"
      ],
      "metadata": {
        "id": "KsB90dFk_NAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=pd.Series(sentences)\n",
        "corpus\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDJtr8Nt_U3v",
        "outputId": "a370c97e-f32f-4f06-ce40-17ef15e9345c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    We are reading about Natural Language Processi...\n",
              "1    Natural Language Processing making computers c...\n",
              "2    The field of Natural Language Processing is ev...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "    '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "\n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "\n",
        "    Output : Returns the cleaned text corpus\n",
        "\n",
        "    '''\n",
        "    cleaned_corpus = pd.Series()\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            if word not in keep_list:\n",
        "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "                p1 = p1.lower()\n",
        "                qs.append(p1)\n",
        "            else : qs.append(word)\n",
        "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
        "    return cleaned_corpus"
      ],
      "metadata": {
        "id": "FxG1cVrP_kCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_removal(corpus):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "5JSH3N4jAX22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "WA7otvGsAtqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type=='snowball':\n",
        "      stemmer=SnowballStemmer(lanaguage='english')\n",
        "      corpus=[[stemmer.stem(x) for x in x ] for x in corpus]\n",
        "    else:\n",
        "      stemmer=PorterStemmer()\n",
        "      corpus=[[stemmer.stem(x) for x in x] for x in corpus]\n"
      ],
      "metadata": {
        "id": "ucKW9CpIA3AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "\n",
        "    Input :\n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should\n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "\n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "\n",
        "    Output : Returns the processed text corpus\n",
        "\n",
        "    '''\n",
        "\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus, keep_list)\n",
        "\n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "\n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "\n",
        "\n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "-b8fqm30A7He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_dot_words = ['U.S.', 'Mr.', 'Mrs.', 'D.C.']"
      ],
      "metadata": {
        "id": "80BviWVQA_I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing with Lemmatization here\n",
        "preprocessed_corpus = preprocess(corpus, keep_list = common_dot_words, stemming = False, stem_type = None,\n",
        "                                lemmatization = True, remove_stopwords = True)\n",
        "preprocessed_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5txHUG7BCFT",
        "outputId": "5e172544-660f-4579-c6df-9bc36d70af39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-a7d5d0d93ac1>:11: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  cleaned_corpus = pd.Series()\n",
            "<ipython-input-24-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-24-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-24-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['read natural language process',\n",
              " 'natural language process make computers comprehend language data',\n",
              " 'field natural language process evolve everyday']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the vocabulary\n"
      ],
      "metadata": {
        "id": "I8HHwlGtBM4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_of_words = set()\n",
        "for sentence in preprocessed_corpus:\n",
        "    for word in sentence.split():\n",
        "        set_of_words.add(word)\n",
        "vocab = list(set_of_words)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f4Cbsz0BG5O",
        "outputId": "3c4dc01b-1ee0-4421-e636-a71266f573f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['field', 'read', 'make', 'language', 'comprehend', 'everyday', 'natural', 'process', 'evolve', 'computers', 'data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position = {}\n",
        "for i, token in enumerate(vocab):\n",
        "    position[token] = i\n",
        "print(position)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er4cK338BPM7",
        "outputId": "2fae9fc0-2974-4097-eedf-027b8f799d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'field': 0, 'read': 1, 'make': 2, 'language': 3, 'comprehend': 4, 'everyday': 5, 'natural': 6, 'process': 7, 'evolve': 8, 'computers': 9, 'data': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a bow matrix"
      ],
      "metadata": {
        "id": "_4TKsrKQBUdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix = np.zeros((len(preprocessed_corpus), len(vocab)))"
      ],
      "metadata": {
        "id": "CzUeuUqQBRpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, preprocessed_sentence in enumerate(preprocessed_corpus):\n",
        "    for token in preprocessed_sentence.split():\n",
        "        bow_matrix[i][position[token]] = bow_matrix[i][position[token]] + 1"
      ],
      "metadata": {
        "id": "wh6ff-i0Ba36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-53ogLPjBde9",
        "outputId": "4d018389-d8c3-4570-eea6-2a1d457ac974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 2., 1., 0., 1., 1., 0., 1., 1.],\n",
              "       [1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Countvectorizer for all the above process"
      ],
      "metadata": {
        "id": "2B9Kwah-KBwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=CountVectorizer()\n",
        "bow_matrix=vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "1Tk3miP5BgFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBfSM6R_KNiz",
        "outputId": "d08d0eb1-3f2a-4b0a-befa-4a8861d211c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x11 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 17 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__v4pnXaKgNb",
        "outputId": "93262d2f-cd3c-4f57-f057-3d2d36b90479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1],\n",
              "       [1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 0],\n",
              "       [0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "bow_matrix.toarray()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rQnTkeVKkjq",
        "outputId": "219a3e55-e49c-4cb0-c216-4ebd1a9a114d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1],\n",
              "       [1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 0],\n",
              "       [0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_ngram_range = CountVectorizer(analyzer='word',ngram_range=(1,3))\n",
        "bow_matrix_ngram =vectorizer_ngram_range.fit_transform(preprocessed_corpus)\n",
        "print(vectorizer_ngram_range.get_feature_names_out())\n",
        "print(bow_matrix_ngram.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfMoa2xyKu3C",
        "outputId": "ab4ac16a-a26d-455a-ec96-cbfb415e27c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'comprehend language' 'comprehend language data' 'computers'\n",
            " 'computers comprehend' 'computers comprehend language' 'data' 'everyday'\n",
            " 'evolve' 'evolve everyday' 'field' 'field natural'\n",
            " 'field natural language' 'language' 'language data' 'language process'\n",
            " 'language process evolve' 'language process make' 'make' 'make computers'\n",
            " 'make computers comprehend' 'natural' 'natural language'\n",
            " 'natural language process' 'process' 'process evolve'\n",
            " 'process evolve everyday' 'process make' 'process make computers' 'read'\n",
            " 'read natural' 'read natural language']\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1]\n",
            " [1 1 1 1 1 1 1 0 0 0 0 0 0 2 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Max features in countvectorizer helps us to keep a cap on the number of features that can be used. Keep in mind that increasing the number of dimension can lead to overfitting which is also known as the curse of dimensionality"
      ],
      "metadata": {
        "id": "IBw1Cs10LwiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_ngram_range = CountVectorizer(analyzer='word',ngram_range=(1,3),max_features=6)\n",
        "bow_matrix_ngram =vectorizer_ngram_range.fit_transform(preprocessed_corpus)\n",
        "print(vectorizer_ngram_range.get_feature_names_out())\n",
        "print(bow_matrix_ngram.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--lYcL66LbWJ",
        "outputId": "c3ccb95a-10fa-47b9-9b72-256023f3019d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['language' 'language process' 'natural' 'natural language'\n",
            " 'natural language process' 'process']\n",
            "[[1 1 1 1 1 1]\n",
            " [2 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using max_df and min_fdf which will ignore the number of words whose frequency is higher then the max_df as well has ignore the words that occurs in lesser amount then min_df\n"
      ],
      "metadata": {
        "id": "vgWHCjXIMnOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Issue with the bag of words approach\n",
        "\n",
        "\n",
        "*   OOV problem\n",
        "*   Cannot capture the semantic meaning behind the sentences\n",
        "\n"
      ],
      "metadata": {
        "id": "WpSWxznDNOoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KUKMSsdyNviz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)\n"
      ],
      "metadata": {
        "id": "vDBe4s2sMJ50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(tf_idf_matrix.toarray())\n",
        "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVN2qa5xOh6E",
        "outputId": "83e8fed8-dbbe-4211-cc61-5b18e2419b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n",
            " [0.40512186 0.40512186 0.40512186 0.         0.         0.\n",
            "  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n",
            " [0.         0.         0.         0.49711994 0.49711994 0.49711994\n",
            "  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n",
            "\n",
            "The shape of the TF-IDF matrix is:  (3, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVOjIrkKRXP9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}