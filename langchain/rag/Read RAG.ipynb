{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOT12aLjl4hYPqw8bQzTmTt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -q openai"],"metadata":{"id":"1e5hSShn4rmC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q python-dotenv"],"metadata":{"id":"hH3FD-cK4wNs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install langchain openai\n","!pip install -q -U faiss-cpu tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGrSqQPJvyOo","executionInfo":{"status":"ok","timestamp":1697024195196,"user_tz":-345,"elapsed":18510,"user":{"displayName":"Aavash Bhattarai","userId":"02255424583720506701"}},"outputId":"a6aa8a2a-9c99-4be1-8122-b0c95c766c91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.312)\n","Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n","Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.1)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n","Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.43)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"]}]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Open AI API Key:\")"],"metadata":{"id":"OmGjNp1a41Qv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697024316114,"user_tz":-345,"elapsed":8099,"user":{"displayName":"Aavash Bhattarai","userId":"02255424583720506701"}},"outputId":"d94f019a-bb42-43b8-bf53-7ff0d2411d73"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Open AI API Key:··········\n"]}]},{"cell_type":"code","source":["from langchain.document_loaders import WebBaseLoader\n","\n","loader = WebBaseLoader(\"https://karpathy.github.io/2015/05/21/rnn-effectiveness/\").load()\n","\n"],"metadata":{"id":"9kwkmZP_44Ic"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import TokenTextSplitter\n","text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n","chunks = text_splitter.split_documents(loader)\n","\n"],"metadata":{"id":"_D28kG414-Nq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.embeddings import CacheBackedEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.storage import LocalFileStore\n","\n","store = LocalFileStore(\"./cachce/\")\n","\n","# create an embedder\n","core_embeddings_model = OpenAIEmbeddings()\n","\n","embedder = CacheBackedEmbeddings.from_bytes_store(\n","    core_embeddings_model,\n","    store,\n","    namespace = core_embeddings_model.model\n",")\n","\n","# store embeddings in vector store\n","vectorstore = FAISS.from_documents(chunks, embedder)\n","\n","\n","# instantiate a retriever\n","retriever = vectorstore.as_retriever()"],"metadata":{"id":"0LQNKKwEgx5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","from langchain.chains import RetrievalQA\n","from langchain.callbacks import StdOutCallbackHandler"],"metadata":{"id":"Ft0Hpj9Lg4BN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = OpenAIChat()\n","handler =  StdOutCallbackHandler()"],"metadata":{"id":"75rdGiRciOYm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["qa_with_sources_chain = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    retriever=retriever,\n","    callbacks=[handler],\n","    return_source_documents=True\n",")"],"metadata":{"id":"o42nZ7aGiSuc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = qa_with_sources_chain({\"query\":\"What are different RNNs ? \"})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJy8ScpQiaRn","executionInfo":{"status":"ok","timestamp":1697024488297,"user_tz":-345,"elapsed":10930,"user":{"displayName":"Aavash Bhattarai","userId":"02255424583720506701"}},"outputId":"7b5c3496-9cc6-4540-9cb0-483476710d4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}]},{"cell_type":"code","source":["print(response['result'])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FU2mgxmbioXj","executionInfo":{"status":"ok","timestamp":1697024885532,"user_tz":-345,"elapsed":450,"user":{"displayName":"Aavash Bhattarai","userId":"02255424583720506701"}},"outputId":"c6ab13d2-2837-4377-ac8b-1826cfc358de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Different RNNs refer to variations or different types of Recurrent Neural Networks. Some common types of RNNs include Vanilla RNNs, Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs). These different types of RNNs have different architectures and update equations, which can affect their performance and capability in different tasks.\n"]}]},{"cell_type":"code","source":["print(response['source_documents'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ho4ziBNiyPC","executionInfo":{"status":"ok","timestamp":1697024887263,"user_tz":-345,"elapsed":6,"user":{"displayName":"Aavash Bhattarai","userId":"02255424583720506701"}},"outputId":"f70c2874-aa05-44e9-ff7a-2959e43bacfd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Document(page_content=\" RNNs, how they work, why they have become a big deal, we’ve trained an RNN character-level language model on several fun datasets, and we’ve seen where RNNs are going. You can confidently expect a large amount of innovation in the space of RNNs, and I believe they will become a pervasive and critical component to intelligent systems.\\nLastly, to add some meta to this post, I trained an RNN on the source file of this blog post. Unfortunately, at about 46K characters I haven’t written enough data to properly feed the RNN, but the returned sample (generated with low temperature to get a more typical sample) is:\\nI've the RNN with and works, but the computed with program of the \\nRNN with and the computed of the RNN with with and the code\\n\\nYes, the post was about RNN and how well it works, so clearly this works :). See you next time!\\nEDIT (extra links):\\nVideos:\\n\\nI gave a talk on this work at the London Deep Learning meetup (video).\\n\\nDiscussions:\\n\\nHN discussion\\nReddit discussion on r/machinelearning\\nReddit discussion on r/programming\\n\\nReplies:\\n\\nYoav Goldberg compared these RNN results to n-gram maximum likelihood (counting) baseline\\n@nylk trained char-rnn on cooking recipes. They look great!\\n@MrChrisJohnson trained char-rnn on Eminem lyrics and then synthesized a rap song with robotic voice reading it out. Hilarious :)\\n@samim trained char-rnn on Obama Speeches. They look fun!\\nJoão Felipe trained char-rnn irish folk music and sampled music\\nBob Sturm also trained char-rnn on music in ABC notation\\nRNN Bible bot by Maximilien\\nLearning Holiness learning the Bible\\nTerminal.com snapshot that has char-rnn set up and ready to go in a browser-based virtual machine (thanks @samim)\\n\\n\\n\\n\\n\\n\\n\\nPlease enable JavaScript to view the comments powered by Disqus.\\ncomments powered by Disqus\\n\\n\\n\\n\\n\\n\\n\\n\\nAndrej Karpathy blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nkarpathy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nkarpathy\\n\\n\\n\\n\\n\\nMusings of a Computer Scientist.\\n\\n\\n\\n\\n\", metadata={'source': 'https://karpathy.github.io/2015/05/21/rnn-effectiveness/', 'title': 'The Unreasonable Effectiveness of Recurrent Neural Networks', 'description': 'Musings of a Computer Scientist.', 'language': 'No language found.'}), Document(page_content=' becoming pervasive in Computer Vision. For example, we’re seeing RNNs in frame-level video classification, image captioning (also including my own work and many others), video captioning and very recently visual question answering. My personal favorite RNNs in Computer Vision paper is Recurrent Models of Visual Attention, both due to its high-level direction (sequential processing of images with glances) and the low-level modeling (REINFORCE learning rule that is a special case of policy gradient methods in Reinforcement Learning, which allows one to train models that perform non-differentiable computation (taking glances around the image in this case)). I’m confident that this type of hybrid model that consists of a blend of CNN for raw perception coupled with an RNN glance policy on top will become pervasive in perception, especially for more complex tasks that go beyond classifying some objects in plain view.\\nInductive Reasoning, Memories and Attention. Another extremely exciting direction of research is oriented towards addressing the limitations of vanilla recurrent networks. One problem is that RNNs are not inductive: They memorize sequences extremely well, but they don’t necessarily always show convincing signs of generalizing in the correct way (I’ll provide pointers in a bit that make this more concrete). A second issue is they unnecessarily couple their representation size to the amount of computation per step. For instance, if you double the size of the hidden state vector you’d quadruple the amount of FLOPS at each step due to the matrix multiplication. Ideally, we’d like to maintain a huge representation/memory (e.g. containing all of Wikipedia or many intermediate state variables), while maintaining the ability to keep computation per time step fixed.\\nThe first convincing example of moving towards these directions was developed in DeepMind’s Neural Turing Machines paper. This paper sketched a path towards models that can perform read/write operations between large, external memory arrays and a smaller set of memory registers (think of these as our working memory) where the computation happens. Crucially, the NTM paper also featured very interesting memory addressing mechanisms that were implemented with a (soft, and fully-differentiable) attention model. The concept of soft attention has turned out to be a powerful modeling feature and was also featured in Neural Machine Translation by Jointly Learning to Align and Translate for Machine Translation and Memory Networks for (toy) Question Answering. In fact, I’d go as far as to say that\\n\\nThe concept of attention is the most interesting recent architectural innovation in neural networks.\\n\\nNow, I don’t want to dive into too many details but a soft attention scheme for memory addressing is convenient because it keeps the model fully-differentiable, but unfortunately one sacrifices efficiency because everything that can be attended to is attended to (but softly). Think of this as declaring a pointer in C that doesn’t point to a specific address but instead defines an entire distribution over all addresses in the entire memory, and dereferencing the pointer returns a weighted sum of the pointed content (that would be an expensive operation!). This has motivated multiple authors to swap soft attention models for hard attention where one samples a particular chunk of memory to attend to (e.g. a read/write action for some memory cell instead of reading/writing from all cells to some degree). This model is significantly more philosophically appealing, scalable and efficient, but unfortunately it is also non-differentiable. This then calls for use of techniques from the Reinforcement Learning literature (e.g. REINFORCE) where people are perfectly used to the concept of non-differentiable interactions. This is very much ongoing work but these hard attention models have been explored, for example, in Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets, Reinforcement Learning Neural Turing Machines, and Show Attend and Tell.\\nPeople. If you’d like to read up on RNNs I recommend theses from Alex Graves, Ilya Sutskever and Tomas Mikolov. For more about REINFORCE and more generally Reinforcement Learning and policy gradient methods (which REINFORCE is a special case of) David Silver’s class, or one of Pieter Abbeel’s classes.\\nCode. If you’d like to play with training RNNs I hear good things about keras or passage for Theano, the code released with this post for Torch, or this gist for raw numpy code I wrote a while ago that implements an efficient, batched LSTM forward and backward pass. You can also have a look at my numpy-based NeuralTalk which uses an RNN/LSTM to caption images, or maybe this Caffe implementation by Jeff Donahue.\\nConclusion\\nWe’ve learned about', metadata={'source': 'https://karpathy.github.io/2015/05/21/rnn-effectiveness/', 'title': 'The Unreasonable Effectiveness of Recurrent Neural Networks', 'description': 'Musings of a Computer Scientist.', 'language': 'No language found.'}), Document(page_content=\"s are fixed vectors, it is still possible to use this powerful formalism to process them in a sequential manner. For instance, the figure below shows results from two very nice papers from DeepMind. On the left, an algorithm learns a recurrent network policy that steers its attention around an image; In particular, it learns to read out house numbers from left to right (Ba et al.). On the right, a recurrent network generates images of digits by learning to sequentially add color to a canvas (Gregor et al.):\\n\\n\\n\\n\\n\\nLeft: RNN learns to read house numbers. Right: RNN learns to paint house numbers.\\n\\nThe takeaway is that even if your data is not in form of sequences, you can still formulate and train powerful models that learn to process it sequentially. You’re learning stateful programs that process your fixed-sized data.\\nRNN computation. So how do these things work? At the core, RNNs have a deceptively simple API: They accept an input vector x and give you an output vector y. However, crucially this output vector’s contents are influenced not only by the input you just fed in, but also on the entire history of inputs you’ve fed in in the past. Written as a class, the RNN’s API consists of a single step function:\\nrnn = RNN()\\ny = rnn.step(x) # x is an input vector, y is the RNN's output vector\\n\\nThe RNN class has some internal state that it gets to update every time step is called. In the simplest case this state consists of a single hidden vector h. Here is an implementation of the step function in a Vanilla RNN:\\nclass RNN:\\n  # ...\\n  def step(self, x):\\n    # update the hidden state\\n    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\\n    # compute the output vector\\n    y = np.dot(self.W_hy, self.h)\\n    return y\\n\\nThe above specifies the forward pass of a vanilla RNN. This RNN’s parameters are the three matrices W_hh, W_xh, W_hy. The hidden state self.h is initialized with the zero vector. The np.tanh function implements a non-linearity that squashes the activations to the range [-1, 1]. Notice briefly how this works: There are two terms inside of the tanh: one is based on the previous hidden state and one is based on the current input. In numpy np.dot is matrix multiplication. The two intermediates interact with addition, and then get squashed by the tanh into the new state vector. If you’re more comfortable with math notation, we can also write the hidden state update as \\\\( h_t = \\\\tanh ( W_{hh} h_{t-1} + W_{xh} x_t ) \\\\), where tanh is applied elementwise.\\nWe initialize the matrices of the RNN with random numbers and the bulk of work during training goes into finding the matrices that give rise to desirable behavior, as measured with some loss function that expresses your preference to what kinds of outputs y you’d like to see in response to your input sequences x.\\nGoing deep. RNNs are neural networks and everything works monotonically better (if done right) if you put on your deep learning hat and start stacking models up like pancakes. For instance, we can form a 2-layer recurrent network as follows:\\ny1 = rnn1.step(x)\\ny = rnn2.step(y1)\\n\\nIn other words we have two separate RNNs: One RNN is receiving the input vectors and the second RNN is receiving the output of the first RNN as its input. Except neither of these RNNs know or care - it’s all just vectors coming in and going out, and some gradients flowing through each module during backpropagation.\\nGetting fancy. I’d like to briefly mention that in practice most of us use a slightly different formulation than what I presented above called a Long Short-Term Memory (LSTM) network. The LSTM is a particular type of recurrent network that works slightly better in practice, owing to its more powerful update equation and some appealing backpropagation dynamics. I won’t go into details, but everything I’ve said about RNNs stays exactly the same, except the mathematical form for computing the update (the line self.h\", metadata={'source': 'https://karpathy.github.io/2015/05/21/rnn-effectiveness/', 'title': 'The Unreasonable Effectiveness of Recurrent Neural Networks', 'description': 'Musings of a Computer Scientist.', 'language': 'No language found.'}), Document(page_content=\"\\n\\n\\n\\n\\nThe Unreasonable Effectiveness of Recurrent Neural Networks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAndrej Karpathy blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Unreasonable Effectiveness of Recurrent Neural Networks\\nMay 21, 2015\\n\\n\\nThere’s something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience I’ve in fact reached the opposite conclusion). Fast forward about a year: I’m training RNNs all the time and I’ve witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you.\\n\\nWe’ll train RNNs to generate text character by character and ponder the question “how is that even possible?”\\n\\nBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyway?\\nRecurrent Neural Networks\\nSequences. Depending on your background you might be wondering: What makes Recurrent Networks so special? A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). Not only that: These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model). The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both. A few examples may make this more concrete:\\n\\n\\nEach rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state (more on this soon). From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.\\n\\nAs you might expect, the sequence regime of operation is much more powerful compared to fixed networks that are doomed from the get-go by a fixed number of computational steps, and hence also much more appealing for those of us who aspire to build more intelligent systems. Moreover, as we’ll see in a bit, RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector. This can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables. Viewed this way, RNNs essentially describe programs. In fact, it is known that RNNs are Turing-Complete in the sense that they can to simulate arbitrary programs (with proper weights). But similar to universal approximation theorems for neural nets you shouldn’t read too much into this. In fact, forget I said anything.\\n\\nIf training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.\\n\\nSequential processing in absence of sequences. You might be thinking that having sequences as inputs or outputs could be relatively rare, but an important point to realize is that even if your inputs/output\", metadata={'source': 'https://karpathy.github.io/2015/05/21/rnn-effectiveness/', 'title': 'The Unreasonable Effectiveness of Recurrent Neural Networks', 'description': 'Musings of a Computer Scientist.', 'language': 'No language found.'})]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mkGmP1YpPxOO"},"execution_count":null,"outputs":[]}]}